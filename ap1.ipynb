{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate AP1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import pyfolio as pf\n",
    "import empyrical as emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data_nn.xlsx')\n",
    "#df.to_pickle(\"data_nn.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the first column as the date index\n",
    "df.set_index(df.columns[0], inplace=True)\n",
    "\n",
    "# Convert the index to string and then to DatetimeIndex format\n",
    "df.index = pd.to_datetime(df.index.astype(str))\n",
    "\n",
    "# Filter the data for the last ten years\n",
    "df_last_10_years = df.loc[df.index > \"2020-01-02\"]\n",
    "\n",
    "# Apply rolling sum with a window of 252 and require at least 126 non-NaN values\n",
    "df_rolling_sum = df_last_10_years.rolling(window=252, min_periods=int(252//2)).sum()\n",
    "\n",
    "# Forward-fill NaN values, but limit this to a maximum of 5 consecutive fills\n",
    "df_filled = df_last_10_years.ffill(limit=5)\n",
    "\n",
    "# Drop any remaining NaN values that still exist after the forward-fill operation\n",
    "df_cleaned = df_filled.dropna()\n",
    "\n",
    "#return back original name to not interruppt code.\n",
    "df_last_10_years = df_cleaned\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactored_advanced_features(df_returns):\n",
    "    \"\"\"\n",
    "    Refactored computation of advanced financial features to reduce DataFrame fragmentation.\n",
    "    \"\"\"\n",
    "    skew = {}\n",
    "    kurtosis = {}\n",
    "    max_drawdown = {}\n",
    "    volatility = {}\n",
    "    vaR = {}\n",
    "    momentum = {}\n",
    "    avg_return = {}\n",
    "    rsi = {}\n",
    "\n",
    "        \n",
    "        # 1. Skewness\n",
    "    print(\"Skewness\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        skew[window] = df_returns.rolling(window).skew()\n",
    "\n",
    "        # 2. Kurtosis\n",
    "    print(\"Kurtosis\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        kurtosis[window]=df_returns.rolling(window).kurt()\n",
    "    \n",
    "    # 3. Maximum drawdown\n",
    "    print(\"Maximum drawdown\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        max_drawdown[window] = df_returns.rolling(window).apply(emp.max_drawdown, raw=True)\n",
    "    \n",
    "    # 4. Volatility\n",
    "    print(\"Volatility\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        volatility[window] = df_returns.rolling(window).std()*(252**0.5)\n",
    "    \n",
    "    # 5. Value at Risk\n",
    "    print(\"Value at Risk\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        vaR[window] = df_returns.rolling(window).apply(emp.value_at_risk, raw=True)\n",
    "    \n",
    "    # 6. Momentum\n",
    "    print(\"Momentum\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        momentum[window] = df_returns.rolling(window).sum() # ?\n",
    "\n",
    "    print(\"Average Return\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        avg_return[window] = df_returns.rolling(window).mean()\n",
    "    \n",
    "    return skew, kurtosis, max_drawdown, volatility, vaR, momentum, avg_return\n",
    "\n",
    "# This function reduces DataFrame fragmentation by constructing all columns and concatenating them at once.\n",
    "\n",
    "# Läs tommys mex hur de gjorde reversal, sen implementera det. Fixa windows size till vad de hade i rapporten.\n",
    "# skew[20].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and capture the output\n",
    "skew, kurtosis, max_drawdown, volatility, vaR, momentum, avg_return = refactored_advanced_features(df_last_10_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">skew_20</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">avg_return_480</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Equities_0</th>\n",
       "      <th>Equities_1</th>\n",
       "      <th>Equities_2</th>\n",
       "      <th>Equities_3</th>\n",
       "      <th>Equities_4</th>\n",
       "      <th>Equities_5</th>\n",
       "      <th>Equities_6</th>\n",
       "      <th>Equities_7</th>\n",
       "      <th>Equities_8</th>\n",
       "      <th>Equities_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Equity_Sector_1</th>\n",
       "      <th>Equity_Sector_2</th>\n",
       "      <th>Equity_Sector_3</th>\n",
       "      <th>Equity_Sector_4</th>\n",
       "      <th>Equity_Sector_5</th>\n",
       "      <th>Equity_Sector_6</th>\n",
       "      <th>Equity_Sector_7</th>\n",
       "      <th>Equity_Sector_8</th>\n",
       "      <th>Equity_Sector_9</th>\n",
       "      <th>Equity_Sector_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Column1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-02-20</th>\n",
       "      <td>-0.240833</td>\n",
       "      <td>-0.198584</td>\n",
       "      <td>0.748803</td>\n",
       "      <td>-0.034554</td>\n",
       "      <td>0.183471</td>\n",
       "      <td>-0.216372</td>\n",
       "      <td>0.256436</td>\n",
       "      <td>-0.157259</td>\n",
       "      <td>1.524128</td>\n",
       "      <td>0.163826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000610</td>\n",
       "      <td>0.000152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-21</th>\n",
       "      <td>-0.086253</td>\n",
       "      <td>-0.070400</td>\n",
       "      <td>0.773535</td>\n",
       "      <td>0.011176</td>\n",
       "      <td>0.312154</td>\n",
       "      <td>-0.041571</td>\n",
       "      <td>0.221016</td>\n",
       "      <td>-0.162284</td>\n",
       "      <td>1.521845</td>\n",
       "      <td>-0.296421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-22</th>\n",
       "      <td>-0.128589</td>\n",
       "      <td>-0.062873</td>\n",
       "      <td>0.757504</td>\n",
       "      <td>-0.109515</td>\n",
       "      <td>0.415347</td>\n",
       "      <td>0.052482</td>\n",
       "      <td>0.191846</td>\n",
       "      <td>-0.470390</td>\n",
       "      <td>1.486008</td>\n",
       "      <td>-0.301710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-23</th>\n",
       "      <td>-0.023004</td>\n",
       "      <td>0.106261</td>\n",
       "      <td>0.830820</td>\n",
       "      <td>-0.006954</td>\n",
       "      <td>0.265489</td>\n",
       "      <td>0.092309</td>\n",
       "      <td>0.325509</td>\n",
       "      <td>-0.476984</td>\n",
       "      <td>1.454702</td>\n",
       "      <td>-0.274151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000627</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-24</th>\n",
       "      <td>0.050596</td>\n",
       "      <td>0.138392</td>\n",
       "      <td>0.864532</td>\n",
       "      <td>0.148482</td>\n",
       "      <td>0.428164</td>\n",
       "      <td>0.081861</td>\n",
       "      <td>0.420396</td>\n",
       "      <td>-0.548086</td>\n",
       "      <td>1.506494</td>\n",
       "      <td>-0.256232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-0.000647</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3080 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              skew_20                                                         \\\n",
       "           Equities_0 Equities_1 Equities_2 Equities_3 Equities_4 Equities_5   \n",
       "Column1                                                                        \n",
       "2023-02-20  -0.240833  -0.198584   0.748803  -0.034554   0.183471  -0.216372   \n",
       "2023-02-21  -0.086253  -0.070400   0.773535   0.011176   0.312154  -0.041571   \n",
       "2023-02-22  -0.128589  -0.062873   0.757504  -0.109515   0.415347   0.052482   \n",
       "2023-02-23  -0.023004   0.106261   0.830820  -0.006954   0.265489   0.092309   \n",
       "2023-02-24   0.050596   0.138392   0.864532   0.148482   0.428164   0.081861   \n",
       "\n",
       "                                                        ...  avg_return_480  \\\n",
       "           Equities_6 Equities_7 Equities_8 Equities_9  ... Equity_Sector_1   \n",
       "Column1                                                 ...                   \n",
       "2023-02-20   0.256436  -0.157259   1.524128   0.163826  ...        0.000223   \n",
       "2023-02-21   0.221016  -0.162284   1.521845  -0.296421  ...        0.000203   \n",
       "2023-02-22   0.191846  -0.470390   1.486008  -0.301710  ...        0.000189   \n",
       "2023-02-23   0.325509  -0.476984   1.454702  -0.274151  ...        0.000200   \n",
       "2023-02-24   0.420396  -0.548086   1.506494  -0.256232  ...        0.000202   \n",
       "\n",
       "                                                                            \\\n",
       "           Equity_Sector_2 Equity_Sector_3 Equity_Sector_4 Equity_Sector_5   \n",
       "Column1                                                                      \n",
       "2023-02-20        0.001289        0.000093        0.000232        0.000087   \n",
       "2023-02-21        0.001335        0.000089        0.000194        0.000060   \n",
       "2023-02-22        0.001290        0.000056        0.000162        0.000026   \n",
       "2023-02-23        0.001304        0.000074        0.000161        0.000030   \n",
       "2023-02-24        0.001269        0.000031        0.000141        0.000001   \n",
       "\n",
       "                                                                            \\\n",
       "           Equity_Sector_6 Equity_Sector_7 Equity_Sector_8 Equity_Sector_9   \n",
       "Column1                                                                      \n",
       "2023-02-20        0.000005        0.000010       -0.000008       -0.000610   \n",
       "2023-02-21       -0.000026       -0.000004       -0.000074       -0.000646   \n",
       "2023-02-22       -0.000051       -0.000029       -0.000105       -0.000646   \n",
       "2023-02-23       -0.000030        0.000023       -0.000123       -0.000627   \n",
       "2023-02-24       -0.000062        0.000002       -0.000159       -0.000647   \n",
       "\n",
       "                             \n",
       "           Equity_Sector_10  \n",
       "Column1                      \n",
       "2023-02-20         0.000152  \n",
       "2023-02-21         0.000083  \n",
       "2023-02-22         0.000094  \n",
       "2023-02-23         0.000099  \n",
       "2023-02-24         0.000093  \n",
       "\n",
       "[5 rows x 3080 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the feature DataFrames list\n",
    "features_df_list = []\n",
    "\n",
    "# Create individual lists for each feature's DataFrame\n",
    "skew_df_list = [] \n",
    "kurtosis_df_list = []\n",
    "max_drawdown_df_list = []\n",
    "volatility_df_list = []\n",
    "vaR_df_list = []\n",
    "momentum_df_list = []\n",
    "avg_return_df_list = []\n",
    "\n",
    "# Windows configuration\n",
    "windows = [20, 40, 60, 100, 180, 240, 360, 480]\n",
    "\n",
    "# Iterate through each feature dictionary and create a DataFrame\n",
    "for feature_name, feature_dict in [('skew', skew), ('kurtosis', kurtosis), ('max_drawdown', max_drawdown), \n",
    "                                   ('volatility', volatility), ('vaR', vaR), ('momentum', momentum), ('avg_return', avg_return)]:\n",
    "    # Only keep the windows that are present for each feature\n",
    "    relevant_windows = windows if feature_name != 'kurtosis' else windows[:-1]\n",
    "    feature_df = pd.concat({f'{feature_name}_{window}': feature_dict[window] for window in relevant_windows}, axis=1)\n",
    "    \n",
    "    # Append the individual DataFrame to the corresponding feature list\n",
    "    if feature_name == 'skew':\n",
    "        skew_df_list.append(feature_df)\n",
    "    elif feature_name == 'kurtosis':\n",
    "        kurtosis_df_list.append(feature_df)\n",
    "    elif feature_name == 'max_drawdown':\n",
    "        max_drawdown_df_list.append(feature_df)\n",
    "    elif feature_name == 'volatility':\n",
    "        volatility_df_list.append(feature_df)\n",
    "    elif feature_name == 'vaR':\n",
    "        vaR_df_list.append(feature_df)\n",
    "    elif feature_name == 'momentum':\n",
    "        momentum_df_list.append(feature_df)\n",
    "    elif feature_name == 'avg_return':\n",
    "        avg_return_df_list.append(feature_df)\n",
    "    \n",
    "    # Add the DataFrame to the main list\n",
    "    features_df_list.append(feature_df)\n",
    "\n",
    "\n",
    "# Concatenate all feature DataFrames into a single DataFrame\n",
    "features_df = pd.concat(features_df_list, axis=1)\n",
    "\n",
    "# Concatenate all feature DataFrames into a single DataFrame for each feature\n",
    "if len(skew_df_list) > 1:\n",
    "    skew_df = pd.concat(skew_df_list, axis=1)\n",
    "if len(kurtosis_df_list) > 1:\n",
    "    kurtosis_df = pd.concat(kurtosis_df_list, axis=1)\n",
    "if len(max_drawdown_df_list) > 1:\n",
    "    max_drawdown_df = pd.concat(max_drawdown_df_list, axis=1)\n",
    "if len(volatility_df_list) > 1:\n",
    "    volatility_df = pd.concat(volatility_df_list, axis=1)\n",
    "if len(vaR_df_list) > 1:\n",
    "    vaR_df = pd.concat(vaR_df_list, axis=1)\n",
    "if len(momentum_df_list) > 1:\n",
    "    momentum_df = pd.concat(momentum_df_list, axis=1)\n",
    "if len(avg_return_df_list) > 1:\n",
    "    avg_return_df = pd.concat(avg_return_df_list, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# The individual lists for each feature now contain their respective DataFrames\n",
    "# And features_df_list contains all the feature DataFrames\n",
    "# Let's print the first item of each sublist to confirm\n",
    "#print(\"Skew DataFrame:\\n\", skew_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Kurtosis DataFrame:\\n\", kurtosis_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Max Drawdown DataFrame:\\n\", max_drawdown_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Volatility DataFrame:\\n\", volatility_df_list[0].tail(), \"\\n\")\n",
    "#print(\"VaR DataFrame:\\n\", vaR_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Momentum DataFrame:\\n\", momentum_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Average Return DataFrame:\\n\", avg_return_df_list[0].tail(), \"\\n\")\n",
    "\n",
    "# Print the last 5 rows of the combined DataFrame\n",
    "features_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSI(df_returns, window):\n",
    "    \"\"\"\n",
    "    Computes the Relative Strength Index (RSI) for a given window.\n",
    "    \"\"\"\n",
    "    df = df_returns.copy()\n",
    "    df[df >= 0] = 1\n",
    "    df[df < 0] = 0\n",
    "    df = df.rolling(window).mean()*100\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSI skip for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the last RSI value for each window\n",
    "rsi_values = {}\n",
    "\n",
    "# Calculate RSI for each window and store the last value\n",
    "for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "    rsi_df = RSI(df_last_10_years, window)  # df_returns is your DataFrame with returns data\n",
    "    last_rsi_value = rsi_df.iloc[-1]  # Get the last row of the RSI DataFrame\n",
    "    rsi_values[window] = last_rsi_value  # Store it in the dictionary with the window as the key\n",
    "\n",
    "# Print the last RSI value for a 20-day window\n",
    "print(\"Last RSI value for 20-day window:\")\n",
    "print(rsi_values[20])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forming the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date             Asset   skew_20   skew_40   skew_60  skew_100  \\\n",
      "46027 2023-02-24   Equity_Sector_6  0.526400  0.257828  0.081595  0.827986   \n",
      "46028 2023-02-24   Equity_Sector_7  0.256501  0.495935  0.103227  0.486835   \n",
      "46029 2023-02-24   Equity_Sector_8  0.818443  0.520856  0.358629  1.111640   \n",
      "46030 2023-02-24   Equity_Sector_9  1.429924  0.867425  0.432064  0.348426   \n",
      "46031 2023-02-24  Equity_Sector_10 -0.185238 -0.284520 -0.171236  0.250022   \n",
      "\n",
      "       skew_180  skew_240  skew_360  skew_480  ...  momentum_360  \\\n",
      "46027  0.395703  0.093499  0.083147  0.024990  ...     -0.115211   \n",
      "46028  0.442083  0.166631  0.107450  0.082749  ...      0.023760   \n",
      "46029  0.656045  0.191849  0.122967  0.063018  ...     -0.150483   \n",
      "46030  0.190163  0.044191 -0.114346 -0.165398  ...     -0.373065   \n",
      "46031 -0.052785 -0.211851 -0.250760 -0.252899  ...      0.061617   \n",
      "\n",
      "       momentum_480  avg_return_20  avg_return_40  avg_return_60  \\\n",
      "46027     -0.029955      -0.001087       0.001747      -0.000262   \n",
      "46028      0.000942      -0.001726       0.000815      -0.000555   \n",
      "46029     -0.076429      -0.003974      -0.000272      -0.000835   \n",
      "46030     -0.310628      -0.000989       0.003031       0.000819   \n",
      "46031      0.044665      -0.001523      -0.001790      -0.001104   \n",
      "\n",
      "       avg_return_100  avg_return_180  avg_return_240  avg_return_360  \\\n",
      "46027        0.001007        0.000366       -0.000694       -0.000320   \n",
      "46028        0.001526        0.000480       -0.000303        0.000066   \n",
      "46029        0.000666       -0.000225       -0.000870       -0.000418   \n",
      "46030        0.001183        0.000128       -0.000940       -0.001036   \n",
      "46031        0.000477        0.000240       -0.000248        0.000171   \n",
      "\n",
      "       avg_return_480  \n",
      "46027       -0.000062  \n",
      "46028        0.000002  \n",
      "46029       -0.000159  \n",
      "46030       -0.000647  \n",
      "46031        0.000093  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the assets and windows outside of the function for clarity\n",
    "assets = [\n",
    "    'Equities_0', 'Equities_1', 'Equities_2', 'Equities_3', 'Equities_4', 'Equities_5', 'Equities_6', 'Equities_7',\n",
    "    'Equities_8', 'Equities_9', 'Equities_10', 'Equities_11', 'Equities_12', 'Equities_13', 'Equities_14', 'Equities_15',\n",
    "    'Equities_16', 'FX_0', 'FX_1', 'FX_2', 'FX_3', 'FX_4', 'FX_5', 'FX_6', 'FX_7', 'FX_8', 'FX_9', 'FX_10', 'FX_11',\n",
    "    'FX_12', 'FX_13', 'Bonds_0', 'Bonds_1', 'Bonds_2', 'Bonds_3', 'Bonds_4', 'Bonds_5', 'Bonds_6', 'Bonds_7', 'Bonds_8',\n",
    "    'Bonds_9', 'Bonds_10', 'Bonds_11', 'Bonds_12', 'Bonds_13', 'Equity_Sector_0', 'Equity_Sector_1', 'Equity_Sector_2',\n",
    "    'Equity_Sector_3', 'Equity_Sector_4', 'Equity_Sector_5', 'Equity_Sector_6', 'Equity_Sector_7', 'Equity_Sector_8',\n",
    "    'Equity_Sector_9', 'Equity_Sector_10'\n",
    "]\n",
    "windows = [20, 40, 60, 100, 180, 240, 360, 480]\n",
    "\n",
    "# Generate the final DataFrame\n",
    "final_rows = []\n",
    "for date in df_last_10_years.index:\n",
    "    for asset in assets:\n",
    "        row = [date, asset]\n",
    "        for feature_name, feature_dict in [('skew', skew), ('kurtosis', kurtosis), ('max_drawdown', max_drawdown), \n",
    "                                           ('volatility', volatility), ('vaR', vaR), ('momentum', momentum), \n",
    "                                           ('avg_return', avg_return)]:\n",
    "            for window in windows:\n",
    "                # Check if the window exists for this feature, if not, use NaN\n",
    "                value = feature_dict[window].loc[date, asset] if window in feature_dict else float('nan')\n",
    "                row.append(value)\n",
    "        final_rows.append(row)\n",
    "\n",
    "# Define the column names for the final DataFrame\n",
    "column_names = ['Date', 'Asset']\n",
    "for feature_name in ['skew', 'kurtosis', 'max_drawdown', 'volatility', 'vaR', 'momentum', 'avg_return']:\n",
    "    for window in windows:\n",
    "        column_names.extend([f'{feature_name}_{window}'])\n",
    "\n",
    "# Now create the DataFrame\n",
    "final_df = pd.DataFrame(final_rows, columns=column_names)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "print(final_df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN - model free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wn/5q1p8b8n0992yr7qgp1s8pv00000gn/T/ipykernel_4863/1725631932.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.hidden_layer(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: -2.028352975845337\n",
      "Epoch 20/50, Loss: -4.184818267822266\n",
      "Epoch 30/50, Loss: -6.348658561706543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 85.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Loss: -8.84572696685791\n",
      "Epoch 50/50, Loss: -14.55651569366455\n",
      "Output Layer Weights: [[ 0.06030712  0.06034895  0.06021939 ...  0.04485895 -0.04009866\n",
      "   0.10348577]\n",
      " [ 0.07706785  0.02154174 -0.12302779 ... -0.00113791  0.12559265\n",
      "   0.1308346 ]\n",
      " [ 0.06152293  0.12568976 -0.00150169 ...  0.01377312  0.02661596\n",
      "   0.13587077]\n",
      " ...\n",
      " [ 0.04617379  0.07505186  0.18688197 ...  0.0413616  -0.05733545\n",
      "   0.0743194 ]\n",
      " [ 0.02462356  0.08353397  0.03387302 ...  0.05489655  0.03614821\n",
      "   0.03483846]\n",
      " [ 0.14545876  0.07126361  0.11199585 ... -0.00419998 -0.07389294\n",
      "   0.20505956]]\n",
      "Output Layer Biases: [ 0.01276876  0.02018515 -0.04689267 -0.01022161  0.00157323 -0.01175919\n",
      "  0.02269167  0.0168194   0.00170925  0.0288313  -0.03933667  0.03971092\n",
      " -0.01995482  0.03591099 -0.01149204 -0.02274209 -0.00681519  0.00722248\n",
      "  0.02638999 -0.03805919  0.05720951  0.01127013  0.00728741 -0.04639073\n",
      " -0.04476226  0.00416599 -0.0114892  -0.00848199 -0.01001663  0.02045355\n",
      " -0.0287329   0.00249665 -0.00562704 -0.00253161 -0.05485503  0.05324174\n",
      "  0.02810174  0.02411565  0.01991793 -0.03101749  0.04625834  0.01969283\n",
      "  0.01605993  0.04218898  0.0222334   0.02686664  0.00605292  0.03979729\n",
      " -0.02079294  0.01095507  0.00701024 -0.02665015 -0.03483104 -0.0018287\n",
      " -0.0312595 ]\n",
      "Neural Network Output: [[0.01906373 0.01987922 0.01642779 ... 0.01684218 0.01792261 0.01782887]\n",
      " [0.01809037 0.01859632 0.01828649 ... 0.01843146 0.01854631 0.01724387]\n",
      " [0.01891586 0.01892928 0.01787107 ... 0.01725843 0.01812112 0.01829373]\n",
      " ...\n",
      " [0.01750884 0.01891373 0.01805099 ... 0.01847334 0.01730621 0.01835843]\n",
      " [0.0229107  0.01824074 0.01631443 ... 0.01641346 0.0161924  0.0203841 ]\n",
      " [0.01831028 0.01890873 0.01819093 ... 0.01783226 0.01915376 0.01846738]]\n",
      "Neural Network Output shape: (165, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the neural network model\n",
    "class MultivariateNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultivariateNN, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.input_layer(x))\n",
    "        x = F.softmax(self.hidden_layer(x))\n",
    "        return x\n",
    "\n",
    "# Custom Sharpe Ratio Loss\n",
    "class SharpeRatioLoss(nn.Module):\n",
    "    def __init__(self, risk_free_rate=0):\n",
    "        super(SharpeRatioLoss, self).__init__()\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "\n",
    "    def forward(self, outputs):\n",
    "        expected_return = outputs.mean()\n",
    "        std_dev_return = outputs.std()\n",
    "        sharpe_ratio = (expected_return - self.risk_free_rate) / (std_dev_return + 1e-6)\n",
    "        return -sharpe_ratio\n",
    "\n",
    "# Assuming 'feature_df' is your dataset as a pandas DataFrame\n",
    "# Calculated features must be part of 'feature_df'\n",
    "calculated_features_df = pd.DataFrame(feature_df)\n",
    "features_df = calculated_features_df.fillna(calculated_features_df.mean())\n",
    "\n",
    "#test\n",
    "returns = df_last_10_years.drop(columns=df_last_10_years.columns[0])\n",
    "\n",
    "# Split the data into training and testing sets (considering all columns for y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, returns, test_size=0.2, random_state=42)\n",
    "\n",
    "\"\"\"# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, features_df, test_size=0.2, random_state=42)\n",
    "\"\"\"\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Converting to PyTorch tensor\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "\n",
    "# Initialize the neural network model\n",
    "#model = MultivariateNN(X_train.shape[1], y_train.shape[1])\n",
    "input_dim = X_train_tensor.shape[1] # Specify the number of input features\n",
    "hidden_dim = 32  # Specify the number of neurons in the hidden layer\n",
    "#output_dim = 55  # Specify the number of assets or allocation decisions\n",
    "output_dim = y_train.shape[1]  # Number of columns to predict\n",
    "print(input_dim)\n",
    "model = MultivariateNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = SharpeRatioLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Optional: Print loss every N epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Extracting only the weights (and biases, if needed) of the output layer\n",
    "output_layer_weights = model.hidden_layer.weight.data.cpu().numpy()\n",
    "output_layer_biases = model.hidden_layer.bias.data.cpu().numpy()\n",
    "\n",
    "# You can now use output_layer_weights and output_layer_biases as needed\n",
    "print(\"Output Layer Weights:\", output_layer_weights)\n",
    "print(\"Output Layer Biases:\", output_layer_biases)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "    neural_network_output = model(X_test_tensor).cpu().numpy()\n",
    "# Now neural_network_output contains the output of the neural network\n",
    "print(\"Neural Network Output:\", neural_network_output)\n",
    "##print the dimensions of neural network\n",
    "print(\"Neural Network Output shape:\", neural_network_output.shape)\n",
    "#with torch.no_grad():\n",
    "    #train_outputs = model(X_train_tensor)\n",
    "    #test_outputs = model(X_test_tensor)\n",
    "    #train_loss = criterion(train_outputs)\n",
    "    #test_loss = criterion(test_outputs)\n",
    "    #print(f\"Final Training Loss: {train_loss.item()}\")\n",
    "    #print(f\"Final Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN - Model Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wn/5q1p8b8n0992yr7qgp1s8pv00000gn/T/ipykernel_4863/4215322618.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.softmax(self.hidden_layer(x))\n",
      "/Users/jakobamaya-scott/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/jakobamaya-scott/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/numpy/core/_methods.py:121: RuntimeWarning: divide by zero encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/Users/jakobamaya-scott/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/Users/jakobamaya-scott/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/pandas/core/frame.py:10869: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  base_cov = np.cov(mat.T, ddof=ddof)\n",
      "/Users/jakobamaya-scott/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Users/jakobamaya-scott/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train)):\n\u001b[1;32m     93\u001b[0m     initial_allocations \u001b[38;5;241m=\u001b[39m nn_outputs[t]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 94\u001b[0m     optimized_weights\u001b[38;5;241m.\u001b[39mappend(optimize_weights(t, returns, output_dim, initial_allocations))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Calculate the loss based on the optimized weights\u001b[39;00m\n\u001b[1;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(torch\u001b[38;5;241m.\u001b[39mtensor(optimized_weights))\n",
      "Cell \u001b[0;32mIn[177], line 77\u001b[0m, in \u001b[0;36moptimize_weights\u001b[0;34m(t, returns, n_assets, initial_allocations)\u001b[0m\n\u001b[1;32m     72\u001b[0m constraints \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     73\u001b[0m     cp\u001b[38;5;241m.\u001b[39msum(cp\u001b[38;5;241m.\u001b[39mmultiply(b, cp\u001b[38;5;241m.\u001b[39mlog(y))) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m c,\n\u001b[1;32m     74\u001b[0m     y \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-5\u001b[39m\n\u001b[1;32m     75\u001b[0m ]\n\u001b[1;32m     76\u001b[0m problem \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mProblem(objective, constraints)\n\u001b[0;32m---> 77\u001b[0m problem\u001b[38;5;241m.\u001b[39msolve(solver\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39mSCS, qcp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, max_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     78\u001b[0m optimal_weights \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimal_weights\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/cvxpy/problems/problem.py:503\u001b[0m, in \u001b[0;36mProblem.solve\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     solve_func \u001b[39m=\u001b[39m Problem\u001b[39m.\u001b[39m_solve\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m solve_func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/cvxpy/problems/problem.py:1068\u001b[0m, in \u001b[0;36mProblem._solve\u001b[0;34m(self, solver, warm_start, verbose, gp, qcp, requires_grad, enforce_dpp, ignore_dpp, canon_backend, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39mhigh\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m low \u001b[39m*\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m   1066\u001b[0m chain \u001b[39m=\u001b[39m Chain(problem\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, reductions\u001b[39m=\u001b[39mreductions)\n\u001b[1;32m   1067\u001b[0m soln \u001b[39m=\u001b[39m bisection\u001b[39m.\u001b[39mbisect(\n\u001b[0;32m-> 1068\u001b[0m     chain\u001b[39m.\u001b[39mreduce(), solver\u001b[39m=\u001b[39msolver, verbose\u001b[39m=\u001b[39mverbose, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1069\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munpack(chain\u001b[39m.\u001b[39mretrieve(soln))\n\u001b[1;32m   1070\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/cvxpy/reductions/reduction.py:94\u001b[0m, in \u001b[0;36mReduction.reduce\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     92\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mThe reduction was constructed without a Problem.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m problem, retrieval_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_emitted_problem \u001b[39m=\u001b[39m problem\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieval_data \u001b[39m=\u001b[39m retrieval_data\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/cvxpy/reductions/chain.py:76\u001b[0m, in \u001b[0;36mChain.apply\u001b[0;34m(self, problem, verbose)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m     75\u001b[0m         s\u001b[39m.\u001b[39mLOGGER\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mApplying reduction \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m(r)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m     problem, inv \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mapply(problem)\n\u001b[1;32m     77\u001b[0m     inverse_data\u001b[39m.\u001b[39mappend(inv)\n\u001b[1;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m problem, inverse_data\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/cvxpy/reductions/dqcp2dcp/dqcp2dcp.py:113\u001b[0m, in \u001b[0;36mDqcp2Dcp.apply\u001b[0;34m(self, problem)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     t \u001b[39m=\u001b[39m Parameter()\n\u001b[0;32m--> 113\u001b[0m constraints \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_canonicalize_constraint(objective \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m t)\n\u001b[1;32m    115\u001b[0m lazy, real \u001b[39m=\u001b[39m _get_lazy_and_real_constraints(constraints)\n\u001b[1;32m    116\u001b[0m param_problem \u001b[39m=\u001b[39m problems\u001b[39m.\u001b[39mproblem\u001b[39m.\u001b[39mProblem(Minimize(\u001b[39m0\u001b[39m), real)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/cvxpy/reductions/dqcp2dcp/dqcp2dcp.py:203\u001b[0m, in \u001b[0;36mDqcp2Dcp._canonicalize_constraint\u001b[0;34m(self, constr)\u001b[0m\n\u001b[1;32m    201\u001b[0m expr \u001b[39m=\u001b[39m lhs\u001b[39m.\u001b[39margs[idx]\n\u001b[1;32m    202\u001b[0m \u001b[39mif\u001b[39;00m lhs\u001b[39m.\u001b[39mis_incr(idx):\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_canonicalize_constraint(expr \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rhs)\n\u001b[1;32m    204\u001b[0m \u001b[39massert\u001b[39;00m lhs\u001b[39m.\u001b[39mis_decr(idx)\n\u001b[1;32m    205\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_canonicalize_constraint(expr \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m rhs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/cvxpy/reductions/dqcp2dcp/dqcp2dcp.py:181\u001b[0m, in \u001b[0;36mDqcp2Dcp._canonicalize_constraint\u001b[0;34m(self, constr)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[39mreturn\u001b[39;00m [\u001b[39mFalse\u001b[39;00m]\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m constr\u001b[39m.\u001b[39mis_dcp():\n\u001b[0;32m--> 181\u001b[0m     canon_constr, aux_constr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanonicalize_tree(constr)\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m [canon_constr] \u001b[39m+\u001b[39m aux_constr\n\u001b[1;32m    184\u001b[0m \u001b[39m# canonicalize lhs <= rhs\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m# either lhs or rhs is quasiconvex (and not convex)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/cvxpy/reductions/canonicalization.py:99\u001b[0m, in \u001b[0;36mCanonicalization.canonicalize_tree\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m     97\u001b[0m constrs \u001b[39m=\u001b[39m []\n\u001b[1;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m expr\u001b[39m.\u001b[39margs:\n\u001b[0;32m---> 99\u001b[0m     canon_arg, c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanonicalize_tree(arg)\n\u001b[1;32m    100\u001b[0m     canon_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [canon_arg]\n\u001b[1;32m    101\u001b[0m     constrs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m c\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/cvxpy/reductions/canonicalization.py:102\u001b[0m, in \u001b[0;36mCanonicalization.canonicalize_tree\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m    100\u001b[0m         canon_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [canon_arg]\n\u001b[1;32m    101\u001b[0m         constrs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m c\n\u001b[0;32m--> 102\u001b[0m     canon_expr, c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanonicalize_expr(expr, canon_args)\n\u001b[1;32m    103\u001b[0m     constrs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m c\n\u001b[1;32m    104\u001b[0m \u001b[39mreturn\u001b[39;00m canon_expr, constrs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/cvxpy/reductions/canonicalization.py:113\u001b[0m, in \u001b[0;36mCanonicalization.canonicalize_expr\u001b[0;34m(self, expr, args)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[39mreturn\u001b[39;00m expr, []\n\u001b[1;32m    112\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(expr) \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanon_methods:\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanon_methods[\u001b[39mtype\u001b[39m(expr)](expr, args)\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m expr\u001b[39m.\u001b[39mcopy(args), []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/cvxpy/reductions/dcp2cone/canonicalizers/quad_form_canon.py:27\u001b[0m, in \u001b[0;36mquad_form_canon\u001b[0;34m(expr, args)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquad_form_canon\u001b[39m(expr, args):\n\u001b[1;32m     26\u001b[0m     \u001b[39m# TODO this doesn't work with parameters!\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     scale, M1, M2 \u001b[39m=\u001b[39m decomp_quad(args[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mvalue)\n\u001b[1;32m     28\u001b[0m     \u001b[39m# Special case where P == 0.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[39mif\u001b[39;00m M1\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m M2\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/cvxpy/atoms/quad_form.py:210\u001b[0m, in \u001b[0;36mdecomp_quad\u001b[0;34m(P, cond, rcond, lower, check_finite)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m         P \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(P\u001b[39m.\u001b[39mtodense())  \u001b[39m# make dense (needs to happen for eigh).\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m w, V \u001b[39m=\u001b[39m LA\u001b[39m.\u001b[39meigh(P, lower\u001b[39m=\u001b[39mlower, check_finite\u001b[39m=\u001b[39mcheck_finite)\n\u001b[1;32m    212\u001b[0m \u001b[39mif\u001b[39;00m rcond \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    213\u001b[0m     cond \u001b[39m=\u001b[39m rcond\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/scipy/linalg/_decomp.py:460\u001b[0m, in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite, subset_by_index, subset_by_value, driver)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m driver \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m drv_str:\n\u001b[1;32m    457\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is unknown. Possible values are \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    458\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(driver, \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(drv_str[\u001b[39m1\u001b[39m:])))\n\u001b[0;32m--> 460\u001b[0m a1 \u001b[39m=\u001b[39m _asarray_validated(a, check_finite\u001b[39m=\u001b[39mcheck_finite)\n\u001b[1;32m    461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(a1\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m a1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m a1\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[1;32m    462\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mexpected square \u001b[39m\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m matrix\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/scipy/_lib/_util.py:240\u001b[0m, in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mmasked arrays are not supported\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    239\u001b[0m toarray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray_chkfinite \u001b[39mif\u001b[39;00m check_finite \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39masarray\n\u001b[0;32m--> 240\u001b[0m a \u001b[39m=\u001b[39m toarray(a)\n\u001b[1;32m    241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m objects_ok:\n\u001b[1;32m    242\u001b[0m     \u001b[39mif\u001b[39;00m a\u001b[39m.\u001b[39mdtype \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mdtype(\u001b[39m'\u001b[39m\u001b[39mO\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/numpy/lib/function_base.py:630\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    628\u001b[0m a \u001b[39m=\u001b[39m asarray(a, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39morder)\n\u001b[1;32m    629\u001b[0m \u001b[39mif\u001b[39;00m a\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mchar \u001b[39min\u001b[39;00m typecodes[\u001b[39m'\u001b[39m\u001b[39mAllFloat\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(a)\u001b[39m.\u001b[39mall():\n\u001b[0;32m--> 630\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    631\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39marray must not contain infs or NaNs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    632\u001b[0m \u001b[39mreturn\u001b[39;00m a\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Define the neural network model\n",
    "class MultivariateNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultivariateNN, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.input_layer(x))\n",
    "        x = F.softmax(self.hidden_layer(x))\n",
    "        return x\n",
    "\n",
    "# Custom Sharpe Ratio Loss\n",
    "class SharpeRatioLoss(nn.Module):\n",
    "    def __init__(self, risk_free_rate=0):\n",
    "        super(SharpeRatioLoss, self).__init__()\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "\n",
    "    def forward(self, outputs):\n",
    "        expected_return = outputs.mean()\n",
    "        std_dev_return = outputs.std()\n",
    "        sharpe_ratio = (expected_return - self.risk_free_rate) / (std_dev_return + 1e-6)\n",
    "        return -sharpe_ratio\n",
    "\n",
    "calculated_features_df = pd.DataFrame(feature_df)\n",
    "features_df = calculated_features_df.fillna(calculated_features_df.mean())\n",
    "returns = df_last_10_years.drop(columns=df_last_10_years.columns[0])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, returns, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Converting to PyTorch tensor\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "\n",
    "# Initialize the neural network model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 32\n",
    "output_dim = y_train.shape[1]\n",
    "model = MultivariateNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = SharpeRatioLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define the optimization function\n",
    "def optimize_weights(t, returns, n_assets, initial_allocations):\n",
    "    b = np.ones(n_assets) / n_assets  # Equal risk budgeting\n",
    "    c = 1  # Constant for constraint\n",
    "    data_t = returns.iloc[:t]\n",
    "    cov_matrix_values = data_t.cov().values\n",
    "    cov_matrix_values = (cov_matrix_values + cov_matrix_values.T) / 2\n",
    "    y = cp.Variable(shape=n_assets)\n",
    "    objective = cp.Minimize(cp.sqrt(cp.quad_form(y, cp.psd_wrap(cov_matrix_values))))\n",
    "    constraints = [\n",
    "        cp.sum(cp.multiply(b, cp.log(y))) >= c,\n",
    "        y >= 1e-5\n",
    "    ]\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve(solver=cp.SCS, qcp=True, eps=1e-5, max_iters=100)\n",
    "    optimal_weights = y.value\n",
    "    return optimal_weights\n",
    "\n",
    "# Training loop with integrated optimization\n",
    "epochs = 50\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass through the neural network\n",
    "    nn_outputs = model(X_train_tensor)\n",
    "\n",
    "    # Optimize the weights for each time step\n",
    "    optimized_weights = []\n",
    "    for t in range(len(X_train)):\n",
    "        initial_allocations = nn_outputs[t].detach().numpy()\n",
    "        optimized_weights.append(optimize_weights(t, returns, output_dim, initial_allocations))\n",
    "\n",
    "    # Calculate the loss based on the optimized weights\n",
    "    loss = criterion(torch.tensor(optimized_weights))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    neural_network_output = model(X_test_tensor).cpu().numpy()\n",
    "    print(\"Neural Network Output:\", neural_network_output)\n",
    "    print(\"Neural Network Output shape:\", neural_network_output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP1 risk budgetering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 20/154 [01:13<08:27,  3.79s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 41\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (date, optimal_weights)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Precompute any variables that don't change inside the loop\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# ...\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# results = Parallel(n_jobs=-1)(delayed(optimize_weights)(t, returns, n_assets) for t in tqdm(range(54, len(returns))))\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Set up the joblib parallelization with tqdm\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m results \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(delayed(optimize_weights)(t, returns, n_assets) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m54\u001b[39m, \u001b[38;5;28mlen\u001b[39m(returns), \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# After parallelization, recombine the results as necessary\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# For example:\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Create a dictionary with dates as keys and optimal weights as values\u001b[39;00m\n\u001b[1;32m     46\u001b[0m optimal_weights_dict \u001b[38;5;241m=\u001b[39m {date: weights \u001b[38;5;28;01mfor\u001b[39;00m date, weights \u001b[38;5;129;01min\u001b[39;00m results}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "# Assuming df_last_10_years contains the daily returns\n",
    "returns = df_last_10_years.drop(columns=df_last_10_years.columns[0])\n",
    "n_assets = len(returns.columns)\n",
    "# Define a function that will be parallelized\n",
    "def optimize_weights(t, returns, n_assets):\n",
    "    b = np.ones(n_assets) / n_assets  # For example, equal risk budgeting\n",
    "    c = 1\n",
    "    # Code that was originally in your for-loop goes here\n",
    "    # For example:\n",
    "    data_t = returns.iloc[:t]\n",
    "    cov_matrix_values = data_t.cov().values\n",
    "    cov_matrix_values = (cov_matrix_values + cov_matrix_values.T)/2\n",
    "    y = cp.Variable(shape=n_assets)\n",
    "    # Objective function: Minimize the square root of the portfolio variance\n",
    "    objective = cp.Minimize(cp.sqrt(cp.quad_form(y, cp.psd_wrap(cov_matrix_values))))\n",
    "    constraints = [\n",
    "        cp.sum(cp.multiply(b, cp.log(y))) >= c,\n",
    "        y >= 1e-5 #strict inequalities are not allowed\n",
    "    ]\n",
    "    # Formulate the optimization problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    # Solve the problem using a suitable solver\n",
    "    problem.solve(solver=cp.SCS,qcp=True, eps = 1e-5, max_iters  = 100) \n",
    "\n",
    "    # Extract the results\n",
    "    optimal_weights = y.value\n",
    "    date = data_t.index[-1]\n",
    "    # Return the results for this iteration\n",
    "    return (date, optimal_weights)\n",
    "\n",
    "# Precompute any variables that don't change inside the loop\n",
    "# ...\n",
    "\n",
    "# Set up the joblib parallelization\n",
    "# Here, 'range(len(returns))' is the range over which you want to parallelize\n",
    "# results = Parallel(n_jobs=-1)(delayed(optimize_weights)(t, returns, n_assets) for t in tqdm(range(54, len(returns))))\n",
    "# Set up the joblib parallelization with tqdm\n",
    "results = Parallel(n_jobs=-1)(delayed(optimize_weights)(t, returns, n_assets) for t in tqdm(range(54, len(returns), 5))\n",
    ")\n",
    "# After parallelization, recombine the results as necessary\n",
    "# For example:\n",
    "# Create a dictionary with dates as keys and optimal weights as values\n",
    "optimal_weights_dict = {date: weights for date, weights in results}\n",
    "\n",
    "w = pd.DataFrame.from_dict(optimal_weights_dict, orient='index', columns=returns.columns)\n",
    "# Normalize the weights\n",
    "w = w.div(w.sum(axis=1), axis=0)\n",
    "# Calculate the portfolio returns\n",
    "portfolio_returns = w.shift(1).mul(returns).dropna(how='all')\n",
    "portfolio_returns.sum(axis=1).cumsum().plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('AP1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41ccadffad69fa272f24ffcf66569b4dc0b054e9043de66efe3af05dc5371c78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
