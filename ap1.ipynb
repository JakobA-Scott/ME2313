{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate AP1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import pyfolio as pf\n",
    "import empyrical as emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data_nn.xlsx')\n",
    "#df.to_pickle(\"data_nn.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the first column as the date index\n",
    "df.set_index(df.columns[0], inplace=True)\n",
    "\n",
    "# Convert the index to string and then to DatetimeIndex format\n",
    "df.index = pd.to_datetime(df.index.astype(str))\n",
    "\n",
    "# Filter the data for the last ten years\n",
    "df_last_10_years = df.loc[df.index > \"2020-01-02\"]\n",
    "\n",
    "# Apply rolling sum with a window of 252 and require at least 126 non-NaN values\n",
    "df_rolling_sum = df_last_10_years.rolling(window=252, min_periods=int(252//2)).sum()\n",
    "\n",
    "# Forward-fill NaN values, but limit this to a maximum of 5 consecutive fills\n",
    "df_filled = df_last_10_years.ffill(limit=5)\n",
    "\n",
    "# Drop any remaining NaN values that still exist after the forward-fill operation\n",
    "df_cleaned = df_filled.dropna()\n",
    "\n",
    "#return back original name to not interruppt code.\n",
    "df_last_10_years = df_cleaned\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactored_advanced_features(df_returns):\n",
    "    \"\"\"\n",
    "    Refactored computation of advanced financial features to reduce DataFrame fragmentation.\n",
    "    \"\"\"\n",
    "    skew = {}\n",
    "    kurtosis = {}\n",
    "    max_drawdown = {}\n",
    "    volatility = {}\n",
    "    vaR = {}\n",
    "    momentum = {}\n",
    "    avg_return = {}\n",
    "    rsi = {}\n",
    "\n",
    "        \n",
    "        # 1. Skewness\n",
    "    print(\"Skewness\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        skew[window] = df_returns.rolling(window).skew()\n",
    "\n",
    "        # 2. Kurtosis\n",
    "    print(\"Kurtosis\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        kurtosis[window]=df_returns.rolling(window).kurt()\n",
    "    \n",
    "    # 3. Maximum drawdown\n",
    "    print(\"Maximum drawdown\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        max_drawdown[window] = df_returns.rolling(window).apply(emp.max_drawdown, raw=True)\n",
    "    \n",
    "    # 4. Volatility\n",
    "    print(\"Volatility\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        volatility[window] = df_returns.rolling(window).std()*(252**0.5)\n",
    "    \n",
    "    # 5. Value at Risk\n",
    "    print(\"Value at Risk\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        vaR[window] = df_returns.rolling(window).apply(emp.value_at_risk, raw=True)\n",
    "    \n",
    "    # 6. Momentum\n",
    "    print(\"Momentum\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        momentum[window] = df_returns.rolling(window).sum() # ?\n",
    "\n",
    "    print(\"Average Return\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        avg_return[window] = df_returns.rolling(window).mean()\n",
    "    \n",
    "    return skew, kurtosis, max_drawdown, volatility, vaR, momentum, avg_return\n",
    "\n",
    "# This function reduces DataFrame fragmentation by constructing all columns and concatenating them at once.\n",
    "\n",
    "# LÃ¤s tommys mex hur de gjorde reversal, sen implementera det. Fixa windows size till vad de hade i rapporten.\n",
    "# skew[20].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness\n",
      "Kurtosis\n",
      "Maximum drawdown\n",
      "Volatility\n",
      "Value at Risk\n",
      "Momentum\n",
      "Average Return\n"
     ]
    }
   ],
   "source": [
    "# Call the function and capture the output\n",
    "skew, kurtosis, max_drawdown, volatility, vaR, momentum, avg_return = refactored_advanced_features(df_last_10_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the feature DataFrames list\n",
    "features_df_list = []\n",
    "\n",
    "# Create individual lists for each feature's DataFrame\n",
    "skew_df_list = [] \n",
    "kurtosis_df_list = []\n",
    "max_drawdown_df_list = []\n",
    "volatility_df_list = []\n",
    "vaR_df_list = []\n",
    "momentum_df_list = []\n",
    "avg_return_df_list = []\n",
    "\n",
    "# Windows configuration\n",
    "windows = [20, 40, 60, 100, 180, 240, 360, 480]\n",
    "\n",
    "# Iterate through each feature dictionary and create a DataFrame\n",
    "for feature_name, feature_dict in [('skew', skew), ('kurtosis', kurtosis), ('max_drawdown', max_drawdown), \n",
    "                                   ('volatility', volatility), ('vaR', vaR), ('momentum', momentum), ('avg_return', avg_return)]:\n",
    "    # Only keep the windows that are present for each feature\n",
    "    relevant_windows = windows if feature_name != 'kurtosis' else windows[:-1]\n",
    "    feature_df = pd.concat({f'{feature_name}_{window}': feature_dict[window] for window in relevant_windows}, axis=1)\n",
    "    \n",
    "    # Append the individual DataFrame to the corresponding feature list\n",
    "    if feature_name == 'skew':\n",
    "        skew_df_list.append(feature_df)\n",
    "    elif feature_name == 'kurtosis':\n",
    "        kurtosis_df_list.append(feature_df)\n",
    "    elif feature_name == 'max_drawdown':\n",
    "        max_drawdown_df_list.append(feature_df)\n",
    "    elif feature_name == 'volatility':\n",
    "        volatility_df_list.append(feature_df)\n",
    "    elif feature_name == 'vaR':\n",
    "        vaR_df_list.append(feature_df)\n",
    "    elif feature_name == 'momentum':\n",
    "        momentum_df_list.append(feature_df)\n",
    "    elif feature_name == 'avg_return':\n",
    "        avg_return_df_list.append(feature_df)\n",
    "    \n",
    "    # Add the DataFrame to the main list\n",
    "    features_df_list.append(feature_df)\n",
    "\n",
    "\n",
    "# Concatenate all feature DataFrames into a single DataFrame\n",
    "features_df = pd.concat(features_df_list, axis=1)\n",
    "\n",
    "# Concatenate all feature DataFrames into a single DataFrame for each feature\n",
    "if len(skew_df_list) > 1:\n",
    "    skew_df = pd.concat(skew_df_list, axis=1)\n",
    "if len(kurtosis_df_list) > 1:\n",
    "    kurtosis_df = pd.concat(kurtosis_df_list, axis=1)\n",
    "if len(max_drawdown_df_list) > 1:\n",
    "    max_drawdown_df = pd.concat(max_drawdown_df_list, axis=1)\n",
    "if len(volatility_df_list) > 1:\n",
    "    volatility_df = pd.concat(volatility_df_list, axis=1)\n",
    "if len(vaR_df_list) > 1:\n",
    "    vaR_df = pd.concat(vaR_df_list, axis=1)\n",
    "if len(momentum_df_list) > 1:\n",
    "    momentum_df = pd.concat(momentum_df_list, axis=1)\n",
    "if len(avg_return_df_list) > 1:\n",
    "    avg_return_df = pd.concat(avg_return_df_list, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# The individual lists for each feature now contain their respective DataFrames\n",
    "# And features_df_list contains all the feature DataFrames\n",
    "# Let's print the first item of each sublist to confirm\n",
    "#print(\"Skew DataFrame:\\n\", skew_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Kurtosis DataFrame:\\n\", kurtosis_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Max Drawdown DataFrame:\\n\", max_drawdown_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Volatility DataFrame:\\n\", volatility_df_list[0].tail(), \"\\n\")\n",
    "#print(\"VaR DataFrame:\\n\", vaR_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Momentum DataFrame:\\n\", momentum_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Average Return DataFrame:\\n\", avg_return_df_list[0].tail(), \"\\n\")\n",
    "\n",
    "# Print the last 5 rows of the combined DataFrame\n",
    "features_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSI(df_returns, window):\n",
    "    \"\"\"\n",
    "    Computes the Relative Strength Index (RSI) for a given window.\n",
    "    \"\"\"\n",
    "    df = df_returns.copy()\n",
    "    df[df >= 0] = 1\n",
    "    df[df < 0] = 0\n",
    "    df = df.rolling(window).mean()*100\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSI skip for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the last RSI value for each window\n",
    "rsi_values = {}\n",
    "\n",
    "# Calculate RSI for each window and store the last value\n",
    "for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "    rsi_df = RSI(df_last_10_years, window)  # df_returns is your DataFrame with returns data\n",
    "    last_rsi_value = rsi_df.iloc[-1]  # Get the last row of the RSI DataFrame\n",
    "    rsi_values[window] = last_rsi_value  # Store it in the dictionary with the window as the key\n",
    "\n",
    "# Print the last RSI value for a 20-day window\n",
    "print(\"Last RSI value for 20-day window:\")\n",
    "print(rsi_values[20])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forming the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the assets and windows outside of the function for clarity\n",
    "assets = [\n",
    "    'Equities_0', 'Equities_1', 'Equities_2', 'Equities_3', 'Equities_4', 'Equities_5', 'Equities_6', 'Equities_7',\n",
    "    'Equities_8', 'Equities_9', 'Equities_10', 'Equities_11', 'Equities_12', 'Equities_13', 'Equities_14', 'Equities_15',\n",
    "    'Equities_16', 'FX_0', 'FX_1', 'FX_2', 'FX_3', 'FX_4', 'FX_5', 'FX_6', 'FX_7', 'FX_8', 'FX_9', 'FX_10', 'FX_11',\n",
    "    'FX_12', 'FX_13', 'Bonds_0', 'Bonds_1', 'Bonds_2', 'Bonds_3', 'Bonds_4', 'Bonds_5', 'Bonds_6', 'Bonds_7', 'Bonds_8',\n",
    "    'Bonds_9', 'Bonds_10', 'Bonds_11', 'Bonds_12', 'Bonds_13', 'Equity_Sector_0', 'Equity_Sector_1', 'Equity_Sector_2',\n",
    "    'Equity_Sector_3', 'Equity_Sector_4', 'Equity_Sector_5', 'Equity_Sector_6', 'Equity_Sector_7', 'Equity_Sector_8',\n",
    "    'Equity_Sector_9', 'Equity_Sector_10'\n",
    "]\n",
    "windows = [20, 40, 60, 100, 180, 240, 360, 480]\n",
    "\n",
    "# Generate the final DataFrame\n",
    "final_rows = []\n",
    "for date in df_last_10_years.index:\n",
    "    for asset in assets:\n",
    "        row = [date, asset]\n",
    "        for feature_name, feature_dict in [('skew', skew), ('kurtosis', kurtosis), ('max_drawdown', max_drawdown), \n",
    "                                           ('volatility', volatility), ('vaR', vaR), ('momentum', momentum), \n",
    "                                           ('avg_return', avg_return)]:\n",
    "            for window in windows:\n",
    "                # Check if the window exists for this feature, if not, use NaN\n",
    "                value = feature_dict[window].loc[date, asset] if window in feature_dict else float('nan')\n",
    "                row.append(value)\n",
    "        final_rows.append(row)\n",
    "\n",
    "# Define the column names for the final DataFrame\n",
    "column_names = ['Date', 'Asset']\n",
    "for feature_name in ['skew', 'kurtosis', 'max_drawdown', 'volatility', 'vaR', 'momentum', 'avg_return']:\n",
    "    for window in windows:\n",
    "        column_names.extend([f'{feature_name}_{window}'])\n",
    "\n",
    "# Now create the DataFrame\n",
    "final_df = pd.DataFrame(final_rows, columns=column_names)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "print(final_df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN - model free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the neural network model\n",
    "class MultivariateNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultivariateNN, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.input_layer(x))\n",
    "        x = F.softmax(self.hidden_layer(x))\n",
    "        return x\n",
    "\n",
    "# Custom Sharpe Ratio Loss\n",
    "class SharpeRatioLoss(nn.Module):\n",
    "    def __init__(self, risk_free_rate=0):\n",
    "        super(SharpeRatioLoss, self).__init__()\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "\n",
    "    def forward(self, outputs):\n",
    "        expected_return = outputs.mean()\n",
    "        std_dev_return = outputs.std()\n",
    "        sharpe_ratio = (expected_return - self.risk_free_rate) / (std_dev_return + 1e-6)\n",
    "        return -sharpe_ratio\n",
    "\n",
    "# Assuming 'feature_df' is your dataset as a pandas DataFrame\n",
    "# Calculated features must be part of 'feature_df'\n",
    "calculated_features_df = pd.DataFrame(feature_df)\n",
    "features_df = calculated_features_df.fillna(calculated_features_df.mean())\n",
    "\n",
    "#test\n",
    "returns = df_last_10_years.drop(columns=df_last_10_years.columns[0])\n",
    "\n",
    "# Split the data into training and testing sets (considering all columns for y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, returns, test_size=0.2, random_state=42)\n",
    "\n",
    "\"\"\"# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, features_df, test_size=0.2, random_state=42)\n",
    "\"\"\"\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Converting to PyTorch tensor\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "\n",
    "# Initialize the neural network model\n",
    "#model = MultivariateNN(X_train.shape[1], y_train.shape[1])\n",
    "input_dim = X_train_tensor.shape[1] # Specify the number of input features\n",
    "hidden_dim = 32  # Specify the number of neurons in the hidden layer\n",
    "#output_dim = 55  # Specify the number of assets or allocation decisions\n",
    "output_dim = y_train.shape[1]  # Number of columns to predict\n",
    "print(input_dim)\n",
    "model = MultivariateNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = SharpeRatioLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Optional: Print loss every N epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Extracting only the weights (and biases, if needed) of the output layer\n",
    "output_layer_weights = model.hidden_layer.weight.data.cpu().numpy()\n",
    "output_layer_biases = model.hidden_layer.bias.data.cpu().numpy()\n",
    "\n",
    "# You can now use output_layer_weights and output_layer_biases as needed\n",
    "print(\"Output Layer Weights:\", output_layer_weights)\n",
    "print(\"Output Layer Biases:\", output_layer_biases)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "    neural_network_output = model(X_test_tensor).cpu().numpy()\n",
    "# Now neural_network_output contains the output of the neural network\n",
    "print(\"Neural Network Output:\", neural_network_output)\n",
    "##print the dimensions of neural network\n",
    "print(\"Neural Network Output shape:\", neural_network_output.shape)\n",
    "#with torch.no_grad():\n",
    "    #train_outputs = model(X_train_tensor)\n",
    "    #test_outputs = model(X_test_tensor)\n",
    "    #train_loss = criterion(train_outputs)\n",
    "    #test_loss = criterion(test_outputs)\n",
    "    #print(f\"Final Training Loss: {train_loss.item()}\")\n",
    "    #print(f\"Final Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN - Model Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: -147.55477905273438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Loss: -147.55477905273438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Loss: -147.55477905273438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Loss: -147.55477905273438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 50/50 [00:17<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Loss: -147.55477905273438\n",
      "Output Layer Weights: [[-0.03926107 -0.03149585 -0.01067632 ... -0.07414781  0.11678757\n",
      "  -0.04233276]\n",
      " [-0.0853817   0.00608076 -0.06463261 ... -0.01583387 -0.03758642\n",
      "   0.04945906]\n",
      " [-0.09923556 -0.05297274  0.01005682 ... -0.0130987   0.02328408\n",
      "   0.00439699]\n",
      " ...\n",
      " [-0.01822511 -0.10091434  0.05556303 ... -0.02164624  0.11646841\n",
      "  -0.04069964]\n",
      " [-0.12611622  0.03442544  0.06425689 ... -0.03670698  0.108124\n",
      "  -0.01735552]\n",
      " [ 0.06869447 -0.12942438 -0.03528391 ... -0.06082166  0.1267455\n",
      "  -0.10428334]]\n",
      "Output Layer Biases: [ 0.09181866  0.05659966  0.11185624 -0.07248983  0.0737378   0.07338402\n",
      "  0.12331356 -0.12485839  0.09854452  0.12545665  0.1263534   0.13361032\n",
      " -0.0551535  -0.03132666 -0.07454468  0.13317142  0.08288682 -0.10143816\n",
      "  0.11570497  0.11284231  0.04390843  0.1344438   0.13156681 -0.03462477\n",
      "  0.05897202 -0.04676094  0.11596219 -0.05476809  0.00488248 -0.04351601\n",
      "  0.03063312  0.01747638  0.00758056  0.10790892  0.04276952  0.09037064\n",
      " -0.00162552  0.09011309 -0.04214821  0.12569968 -0.05375116  0.11857377\n",
      "  0.01052397  0.05084434  0.03434627 -0.0903302  -0.00133763 -0.07056384\n",
      " -0.13136522  0.07826331 -0.11947682 -0.06637753  0.09791006 -0.13393848\n",
      " -0.06991524]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Output: [0.01804948 0.01812409 0.01820403 0.01817929 0.01820585 0.01808591\n",
      " 0.0182464  0.01819765 0.01811458 0.01818441 0.01810397 0.0182211\n",
      " 0.01823818 0.01818087 0.01811183 0.01823247 0.01759337 0.01823489\n",
      " 0.01827067 0.01837297 0.01807944 0.01805247 0.01808746 0.01833909\n",
      " 0.01810963 0.01817504 0.01808747 0.01807189 0.01815466 0.01838644\n",
      " 0.01821834 0.0181925  0.01830699 0.01815723 0.01820097 0.01808427\n",
      " 0.01813412 0.01822197 0.01815492 0.01821693 0.01833662 0.01821991\n",
      " 0.01813872 0.01814932 0.01825572 0.01817208 0.01833373 0.01824975\n",
      " 0.01825392 0.0182912  0.01828905 0.0182871  0.01806469 0.0181758\n",
      " 0.01819847]\n",
      "Neural Network Output shape: (55,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "class RiskBudgetingLayer(nn.Module):\n",
    "    def __init__(self, n_assets, risk_budgets):\n",
    "        super(RiskBudgetingLayer, self).__init__()\n",
    "        self.n_assets = n_assets\n",
    "        self.risk_budgets = risk_budgets\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate covariance matrix\n",
    "        covariance_matrix = torch.cov(x.T).detach().cpu().numpy()\n",
    "        assert covariance_matrix.shape[0] == covariance_matrix.shape[1], \"Covariance matrix is not square\"\n",
    "\n",
    "        # Ensure positive semi-definiteness of the covariance matrix\n",
    "        covariance_matrix = (covariance_matrix + covariance_matrix.T) / 2\n",
    "        eigenvalues, _ = np.linalg.eigh(covariance_matrix)\n",
    "        covariance_matrix += np.eye(self.n_assets) * np.maximum(0, -eigenvalues.min() + 1e-6)\n",
    "\n",
    "        # Convex optimization\n",
    "        y = cp.Variable(self.n_assets)\n",
    "        objective = cp.Minimize(cp.sqrt(cp.quad_form(y, covariance_matrix)))\n",
    "        constraints = [cp.sum(cp.multiply(self.risk_budgets, cp.log(y))) >= 1, y >= 1e-5]\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        problem.solve(solver=cp.SCS, qcp=True, eps=1e-5, max_iters=100)\n",
    "        optimized_allocation = y.value\n",
    "\n",
    "        # Check for invalid values and handle them\n",
    "        if optimized_allocation is None or np.any(np.isnan(optimized_allocation)):\n",
    "            optimized_allocation = np.ones(self.n_assets) / self.n_assets  # Fallback to equal allocation\n",
    "\n",
    "        # Convert back to PyTorch tensor with a differentiable operation\n",
    "        optimized_allocation_tensor = torch.tensor(optimized_allocation, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "        # Apply softmax to ensure the output sums to 1\n",
    "        return F.softmax(optimized_allocation_tensor * 1.0, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "class MultivariateNNWithRiskBudgeting(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_assets, risk_budgets):\n",
    "        super(MultivariateNNWithRiskBudgeting, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.risk_budgeting_layer = RiskBudgetingLayer(n_assets, risk_budgets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.input_layer(x))\n",
    "        x = F.softmax(self.hidden_layer(x), dim=1)\n",
    "        x = self.risk_budgeting_layer(x)\n",
    "        return x\n",
    "\n",
    "# Assuming the data and necessary variables are already defined as per your notebook\n",
    "# Example usage\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 55\n",
    "n_assets = y_train.shape[1]\n",
    "risk_budgets = np.ones(n_assets) / n_assets  # Example risk budgets, equal for each asset\n",
    "\n",
    "model = MultivariateNNWithRiskBudgeting(input_dim, hidden_dim, n_assets, risk_budgets)\n",
    "\n",
    "# Continue with the existing training loop and loss function\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = SharpeRatioLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Optional: Print loss every N epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Extracting only the weights (and biases, if needed) of the output layer\n",
    "output_layer_weights = model.hidden_layer.weight.data.cpu().numpy()\n",
    "output_layer_biases = model.hidden_layer.bias.data.cpu().numpy()\n",
    "\n",
    "# You can now use output_layer_weights and output_layer_biases as needed\n",
    "print(\"Output Layer Weights:\", output_layer_weights)\n",
    "print(\"Output Layer Biases:\", output_layer_biases)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "    neural_network_output = model(X_test_tensor).cpu().numpy()\n",
    "# Now neural_network_output contains the output of the neural network\n",
    "print(\"Neural Network Output:\", neural_network_output)\n",
    "##print the dimensions of neural network\n",
    "print(\"Neural Network Output shape:\", neural_network_output.shape)\n",
    "#with torch.no_grad():\n",
    "    #train_outputs = model(X_train_tensor)\n",
    "    #test_outputs = model(X_test_tensor)\n",
    "    #train_loss = criterion(train_outputs)\n",
    "    #test_loss = criterion(test_outputs)\n",
    "    #print(f\"Final Training Loss: {train_loss.item()}\")\n",
    "    #print(f\"Final Test Loss: {test_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Risk budgeting benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP1 risk budgetering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "# Assuming df_last_10_years contains the daily returns\n",
    "returns = df_last_10_years.drop(columns=df_last_10_years.columns[0])\n",
    "n_assets = len(returns.columns)\n",
    "# Define a function that will be parallelized\n",
    "def optimize_weights(t, returns, n_assets):\n",
    "    b = np.ones(n_assets) / n_assets  # For example, equal risk budgeting\n",
    "    c = 1\n",
    "    # Code that was originally in your for-loop goes here\n",
    "    # For example:\n",
    "    data_t = returns.iloc[:t]\n",
    "    cov_matrix_values = data_t.cov().values\n",
    "    cov_matrix_values = (cov_matrix_values + cov_matrix_values.T)/2\n",
    "    y = cp.Variable(shape=n_assets)\n",
    "    # Objective function: Minimize the square root of the portfolio variance\n",
    "    objective = cp.Minimize(cp.sqrt(cp.quad_form(y, cp.psd_wrap(cov_matrix_values))))\n",
    "    constraints = [\n",
    "        cp.sum(cp.multiply(b, cp.log(y))) >= c,\n",
    "        y >= 1e-5 #strict inequalities are not allowed\n",
    "    ]\n",
    "    # Formulate the optimization problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    # Solve the problem using a suitable solver\n",
    "    problem.solve(solver=cp.SCS,qcp=True, eps = 1e-5, max_iters  = 100) \n",
    "\n",
    "    # Extract the results\n",
    "    optimal_weights = y.value\n",
    "    date = data_t.index[-1]\n",
    "    # Return the results for this iteration\n",
    "    return (date, optimal_weights)\n",
    "\n",
    "# Precompute any variables that don't change inside the loop\n",
    "# ...\n",
    "\n",
    "# Set up the joblib parallelization\n",
    "# Here, 'range(len(returns))' is the range over which you want to parallelize\n",
    "# results = Parallel(n_jobs=-1)(delayed(optimize_weights)(t, returns, n_assets) for t in tqdm(range(54, len(returns))))\n",
    "# Set up the joblib parallelization with tqdm\n",
    "results = Parallel(n_jobs=-1)(delayed(optimize_weights)(t, returns, n_assets) for t in tqdm(range(54, len(returns), 5))\n",
    ")\n",
    "# After parallelization, recombine the results as necessary\n",
    "# For example:\n",
    "# Create a dictionary with dates as keys and optimal weights as values\n",
    "optimal_weights_dict = {date: weights for date, weights in results}\n",
    "\n",
    "w = pd.DataFrame.from_dict(optimal_weights_dict, orient='index', columns=returns.columns)\n",
    "# Normalize the weights\n",
    "w = w.div(w.sum(axis=1), axis=0)\n",
    "# Calculate the portfolio returns\n",
    "portfolio_returns = w.shift(1).mul(returns).dropna(how='all')\n",
    "portfolio_returns.sum(axis=1).cumsum().plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('AP1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41ccadffad69fa272f24ffcf66569b4dc0b054e9043de66efe3af05dc5371c78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
