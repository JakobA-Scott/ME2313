{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate AP1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import pyfolio as pf\n",
    "import empyrical as emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data_nn.xlsx')\n",
    "#df.to_pickle(\"data_nn.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the first column as the date index\n",
    "df.set_index(df.columns[0], inplace=True)\n",
    "\n",
    "# Convert the index to string and then to DatetimeIndex format\n",
    "df.index = pd.to_datetime(df.index.astype(str))\n",
    "\n",
    "# Filter the data for the last ten years\n",
    "df_last_10_years = df.loc[df.index > \"2020-01-02\"]\n",
    "\n",
    "# Apply rolling sum with a window of 252 and require at least 126 non-NaN values\n",
    "df_rolling_sum = df_last_10_years.rolling(window=252, min_periods=int(252//2)).sum()\n",
    "\n",
    "# Forward-fill NaN values, but limit this to a maximum of 5 consecutive fills\n",
    "df_filled = df_last_10_years.ffill(limit=5)\n",
    "\n",
    "# Drop any remaining NaN values that still exist after the forward-fill operation\n",
    "df_cleaned = df_filled.dropna()\n",
    "\n",
    "#return back original name to not interruppt code.\n",
    "df_last_10_years = df_cleaned\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactored_advanced_features(df_returns):\n",
    "    \"\"\"\n",
    "    Refactored computation of advanced financial features to reduce DataFrame fragmentation.\n",
    "    \"\"\"\n",
    "    skew = {}\n",
    "    kurtosis = {}\n",
    "    max_drawdown = {}\n",
    "    volatility = {}\n",
    "    vaR = {}\n",
    "    momentum = {}\n",
    "    avg_return = {}\n",
    "    rsi = {}\n",
    "\n",
    "        \n",
    "        # 1. Skewness\n",
    "    print(\"Skewness\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        skew[window] = df_returns.rolling(window).skew()\n",
    "\n",
    "        # 2. Kurtosis\n",
    "    print(\"Kurtosis\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        kurtosis[window]=df_returns.rolling(window).kurt()\n",
    "    \n",
    "    # 3. Maximum drawdown\n",
    "    print(\"Maximum drawdown\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        max_drawdown[window] = df_returns.rolling(window).apply(emp.max_drawdown, raw=True)\n",
    "    \n",
    "    # 4. Volatility\n",
    "    print(\"Volatility\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        volatility[window] = df_returns.rolling(window).std()*(252**0.5)\n",
    "    \n",
    "    # 5. Value at Risk\n",
    "    print(\"Value at Risk\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        vaR[window] = df_returns.rolling(window).apply(emp.value_at_risk, raw=True)\n",
    "    \n",
    "    # 6. Momentum\n",
    "    print(\"Momentum\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        momentum[window] = df_returns.rolling(window).sum() # ?\n",
    "\n",
    "    print(\"Average Return\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        avg_return[window] = df_returns.rolling(window).mean()\n",
    "    \n",
    "    return skew, kurtosis, max_drawdown, volatility, vaR, momentum, avg_return\n",
    "\n",
    "# This function reduces DataFrame fragmentation by constructing all columns and concatenating them at once.\n",
    "\n",
    "# LÃ¤s tommys mex hur de gjorde reversal, sen implementera det. Fixa windows size till vad de hade i rapporten.\n",
    "# skew[20].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and capture the output\n",
    "skew, kurtosis, max_drawdown, volatility, vaR, momentum, avg_return = refactored_advanced_features(df_last_10_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the feature DataFrames list\n",
    "features_df_list = []\n",
    "\n",
    "# Create individual lists for each feature's DataFrame\n",
    "skew_df_list = [] \n",
    "kurtosis_df_list = []\n",
    "max_drawdown_df_list = []\n",
    "volatility_df_list = []\n",
    "vaR_df_list = []\n",
    "momentum_df_list = []\n",
    "avg_return_df_list = []\n",
    "\n",
    "# Windows configuration\n",
    "windows = [20, 40, 60, 100, 180, 240, 360, 480]\n",
    "\n",
    "# Iterate through each feature dictionary and create a DataFrame\n",
    "for feature_name, feature_dict in [('skew', skew), ('kurtosis', kurtosis), ('max_drawdown', max_drawdown), \n",
    "                                   ('volatility', volatility), ('vaR', vaR), ('momentum', momentum), ('avg_return', avg_return)]:\n",
    "    # Only keep the windows that are present for each feature\n",
    "    relevant_windows = windows if feature_name != 'kurtosis' else windows[:-1]\n",
    "    feature_df = pd.concat({f'{feature_name}_{window}': feature_dict[window] for window in relevant_windows}, axis=1)\n",
    "    \n",
    "    # Append the individual DataFrame to the corresponding feature list\n",
    "    if feature_name == 'skew':\n",
    "        skew_df_list.append(feature_df)\n",
    "    elif feature_name == 'kurtosis':\n",
    "        kurtosis_df_list.append(feature_df)\n",
    "    elif feature_name == 'max_drawdown':\n",
    "        max_drawdown_df_list.append(feature_df)\n",
    "    elif feature_name == 'volatility':\n",
    "        volatility_df_list.append(feature_df)\n",
    "    elif feature_name == 'vaR':\n",
    "        vaR_df_list.append(feature_df)\n",
    "    elif feature_name == 'momentum':\n",
    "        momentum_df_list.append(feature_df)\n",
    "    elif feature_name == 'avg_return':\n",
    "        avg_return_df_list.append(feature_df)\n",
    "    \n",
    "    # Add the DataFrame to the main list\n",
    "    features_df_list.append(feature_df)\n",
    "\n",
    "# Concatenate all feature DataFrames into a single DataFrame\n",
    "features_df = pd.concat(features_df_list, axis=1)\n",
    "\n",
    "# The individual lists for each feature now contain their respective DataFrames\n",
    "# And features_df_list contains all the feature DataFrames\n",
    "# Let's print the first item of each sublist to confirm\n",
    "#print(\"Skew DataFrame:\\n\", skew_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Kurtosis DataFrame:\\n\", kurtosis_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Max Drawdown DataFrame:\\n\", max_drawdown_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Volatility DataFrame:\\n\", volatility_df_list[0].tail(), \"\\n\")\n",
    "#print(\"VaR DataFrame:\\n\", vaR_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Momentum DataFrame:\\n\", momentum_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Average Return DataFrame:\\n\", avg_return_df_list[0].tail(), \"\\n\")\n",
    "\n",
    "# Print the last 5 rows of the combined DataFrame\n",
    "features_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSI(df_returns, window):\n",
    "    \"\"\"\n",
    "    Computes the Relative Strength Index (RSI) for a given window.\n",
    "    \"\"\"\n",
    "    df = df_returns.copy()\n",
    "    df[df >= 0] = 1\n",
    "    df[df < 0] = 0\n",
    "    df = df.rolling(window).mean()*100\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSI skip for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the last RSI value for each window\n",
    "rsi_values = {}\n",
    "\n",
    "# Calculate RSI for each window and store the last value\n",
    "for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "    rsi_df = RSI(df_last_10_years, window)  # df_returns is your DataFrame with returns data\n",
    "    last_rsi_value = rsi_df.iloc[-1]  # Get the last row of the RSI DataFrame\n",
    "    rsi_values[window] = last_rsi_value  # Store it in the dictionary with the window as the key\n",
    "\n",
    "# Print the last RSI value for a 20-day window\n",
    "print(\"Last RSI value for 20-day window:\")\n",
    "print(rsi_values[20])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: NaN values in training data: 0\n",
      "Debug: Initial weights are [[ 0.01579001  0.04624189  0.0554379  ... -0.0016118  -0.0041359\n",
      "  -0.04692262]\n",
      " [-0.05253757 -0.03971265 -0.05537774 ...  0.06627474  0.0211127\n",
      "   0.05498072]\n",
      " [-0.05942423  0.00161982 -0.06776381 ... -0.04199161 -0.01984524\n",
      "  -0.05122317]\n",
      " ...\n",
      " [ 0.02323836  0.01368109  0.01894289 ...  0.04954439  0.00701638\n",
      "   0.07517029]\n",
      " [-0.08546984 -0.03160863 -0.07787771 ...  0.00322067  0.00729904\n",
      "  -0.00902276]\n",
      " [-0.069322   -0.04149174 -0.07131457 ... -0.07467739 -0.07709451\n",
      "  -0.04498061]]\n",
      "Debug: The first layer weights have a shape of (128, 448).\n",
      "Debug: Feature importance is [6.7518516 6.179517  6.3350725 7.079707  6.2820826 6.712406  6.8396134\n",
      " 7.280704  6.80403   6.4681373 6.6247606 7.0132575 6.8663735 6.9801826\n",
      " 6.755113  7.1682596 6.807978  6.890221  6.580156  6.806979  6.434308\n",
      " 6.44356   6.305599  6.3985233 6.993012  6.864393  7.3122306 7.2732897\n",
      " 6.7891855 6.651632  7.3493924 7.2543397 6.2830815 7.002481  6.634133\n",
      " 7.395404  6.759315  7.3718266 6.864206  7.087146  7.0112824 6.6853795\n",
      " 7.032266  6.9658165 6.59187   6.523373  6.3342514 6.5374117 6.259315\n",
      " 7.088951  7.222289  6.525342  6.533586  7.049617  6.5094504 6.4888616\n",
      " 6.7876663 6.369295  6.7628694 6.3366413 6.743168  6.3866854 6.302364\n",
      " 7.023266  6.2850866 6.249419  6.2155123 7.04182   6.5972543 6.2566957\n",
      " 6.6858463 6.843318  6.7228956 6.647137  6.436888  6.685037  6.392477\n",
      " 6.844525  6.808547  6.944406  6.3947616 6.39836   6.929234  7.3422756\n",
      " 6.0770593 6.716158  6.8104568 6.772861  6.926476  7.050661  7.0007725\n",
      " 6.1447563 7.0581174 7.1104255 6.455978  6.481531  7.1791363 7.1253843\n",
      " 6.8282266 7.084601  7.0655665 6.527178  6.428702  6.849195  6.133928\n",
      " 6.4634304 6.122756  6.6181345 6.300574  6.857234  6.744205  6.1782255\n",
      " 7.034035  7.0993347 6.566191  6.783127  6.7258964 6.9975667 7.048818\n",
      " 6.7906756 6.766492  6.535157  6.363243  6.4568233 6.6041226 6.572974\n",
      " 7.179639  6.727118  6.3577952 7.1622467 7.01112   6.8170004 6.519331\n",
      " 6.9729967 6.420584  6.5828047 6.624552  6.7110963 6.269718  7.0893307\n",
      " 6.6426864 6.87626   6.8756833 7.058446  6.394015  6.511337  7.254892\n",
      " 6.3087244 6.018817  7.1862507 6.994416  6.517852  6.6857142 6.683133\n",
      " 6.8080225 6.8281565 7.4169993 6.706301  6.6321726 6.861883  6.7141542\n",
      " 6.3890734 6.412522  6.900957  6.5805097 6.326125  6.7693925 6.6007595\n",
      " 6.697977  6.7963066 6.847115  6.64102   7.3671007 6.8569713 6.7349267\n",
      " 6.8696966 6.475454  6.845547  6.685398  6.815148  6.8250194 6.554268\n",
      " 7.021306  6.1008353 6.3773055 6.4916487 7.0095224 6.751525  6.27174\n",
      " 6.8490376 6.858654  6.412394  6.4628086 6.2329793 6.169095  7.120865\n",
      " 6.4371247 6.526826  5.9023438 7.116555  6.208886  6.445065  7.260682\n",
      " 6.505585  6.386601  6.048698  6.213353  6.453939  6.7278237 6.819908\n",
      " 6.6269517 6.488215  6.5603585 6.307022  6.523049  6.3052864 6.842038\n",
      " 6.3645577 6.477214  6.8803678 6.378923  6.4835634 6.990147  6.3827605\n",
      " 6.4472785 6.122362  6.280225  5.941108  6.329954  6.3542356 6.4126005\n",
      " 6.436005  6.714612  6.6031566 6.0155954 6.2550287 6.861451  6.2237453\n",
      " 6.713856  6.6177683 6.732564  6.3865123 7.1098332 6.9968495 6.4100785\n",
      " 5.641345  6.5330524 6.562172  6.3603992 6.8871803 7.09825   6.081452\n",
      " 6.1743875 6.4542074 7.009531  6.7709203 6.6224384 5.6534653 6.4408\n",
      " 6.060575  6.437823  6.737096  6.364452  6.682473  6.2014327 6.2664814\n",
      " 6.721315  6.3100924 6.820442  6.47996   6.1817346 6.8301225 6.109552\n",
      " 6.720998  6.5862756 6.199454  6.985384  6.723586  6.2524614 6.2116213\n",
      " 6.1326976 6.2398295 6.1188684 6.65122   6.0831485 6.143502  6.592981\n",
      " 6.0084004 6.4839344 6.4507413 6.0987453 6.2204404 6.416406  6.3712387\n",
      " 6.073169  6.555688  6.121834  6.19095   6.4478602 6.506261  6.502172\n",
      " 5.8857284 6.1184335 6.733292  6.3497677 6.455639  6.9358735 6.362905\n",
      " 6.5888143 6.49887   6.328809  6.165798  6.8608966 6.500769  6.2539587\n",
      " 6.410926  6.396356  6.423442  6.298956  5.7912188 6.110807  6.6855283\n",
      " 6.0095644 7.094819  6.1849527 5.96944   6.4131093 6.2530646 6.3744497\n",
      " 6.1857023 6.2954755 6.683481  6.3181944 6.8925614 5.99366   6.520674\n",
      " 6.6997747 6.754448  6.7611356 6.476376  6.8449802 6.693835  6.279041\n",
      " 5.887741  7.1471457 6.72268   6.325723  6.887117  6.1909933 6.933596\n",
      " 6.3061037 6.553709  6.534117  6.419455  6.3176055 6.608212  6.797091\n",
      " 6.6986547 6.1430116 6.3205686 6.7443924 6.7906795 6.7493086 6.508152\n",
      " 5.9617834 6.5075846 6.177585  6.14758   6.39206   6.723273  6.534252\n",
      " 6.368909  6.41882   6.1371956 6.361579  6.4533963 6.090987  7.005893\n",
      " 6.4615126 6.749058  6.6727476 5.9220695 6.277034  6.26985   6.074362\n",
      " 6.1585236 6.2816215 6.3605547 6.4510136 6.427291  6.160174  6.5996094\n",
      " 6.607513  6.255447  6.21298   6.3541636 6.3416624 6.1863346 6.565647\n",
      " 6.785188  6.538147  5.9971504 6.4896026 6.532084  6.395436  6.2625256\n",
      " 5.9760633 6.954119  6.491033  6.5138197 6.462526  6.272974  6.9088736\n",
      " 6.480662  6.792305  6.4089594 6.697801  6.7176957 6.463122  6.288464\n",
      " 6.628591  6.4486265 6.3820186 6.2772374 6.5729346 6.2543926 6.391114\n",
      " 6.331192  6.768873  5.8643265 6.100785  5.9984975 5.990935  6.4558244\n",
      " 5.954588  7.150805  6.938286  6.2680783 6.5086265 6.20442   6.4121256\n",
      " 5.6386647 6.0842123 6.8554997 5.7988906 6.305836  6.0174713 6.7941127].\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Define the neural network model to accommodate multiple output dimensions\n",
    "class MultivariateNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MultivariateNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)  # Output dimension should match the number of columns\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "# Using actual columns from the dataset to populate 'dataframes_to_concat'\n",
    "# As per the uploaded dataset, 'Equities_0', 'Equities_1', and 'Equities_2' are used\n",
    "# Assume that calculated_features_df is a DataFrame containing all the calculated features\n",
    "#calculated_features = skew, kurtosis, max_drawdown, volatility, vaR, momentum, avg_return\n",
    "calculated_features = feature_df\n",
    "calculated_features_df = pd.DataFrame(calculated_features)\n",
    "dataframes_to_concat = [calculated_features_df]\n",
    "\n",
    "# Concatenate feature data frames \n",
    "features_df = pd.concat(dataframes_to_concat, axis=1)\n",
    "features_df.fillna(features_df.mean(), inplace=True)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets (considering all columns for y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, features_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(f\"Debug: NaN values in training data: {np.isnan(X_train_scaled).sum()}\")\n",
    "\n",
    "# Converting to PyTorch tensor\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values)  # Multiple columns are included\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "\n",
    "# Initialize the neural network model\n",
    "output_dim = y_train.shape[1]  # Number of columns to predict\n",
    "model = MultivariateNN(X_train.shape[1], output_dim)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "initial_weights = model.fc1.weight.data.numpy()\n",
    "print(f\"Debug: Initial weights are {initial_weights}\")\n",
    "\n",
    "# Added check for empty list before DataFrame concatenation\n",
    "\n",
    "# Check if dataframes_to_concat is empty before attempting concatenation\n",
    "if len(dataframes_to_concat) == 0:\n",
    "    print(\"Warning: No DataFrames to concatenate.\")\n",
    "else:\n",
    "    features_df = pd.concat(dataframes_to_concat, axis=1)\n",
    "\n",
    "# Placeholder for Backward Neural Network\n",
    "class BackwardNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(BackwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "        \n",
    "# # Initialize the backward neural network model\n",
    "# backward_model = BackwardNN(X_train.shape[1], output_dim)\n",
    "\n",
    "# # Training loop for backward neural network\n",
    "# for epoch in range(epochs):\n",
    "#     backward_model.train()\n",
    "#     optimizer.zero_grad()\n",
    "#     backward_outputs = backward_model(X_train_tensor)\n",
    "#     backward_loss = criterion(backward_outputs, y_train_tensor)\n",
    "#     backward_loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# After training\n",
    "first_layer_weights = model.fc1.weight.data.numpy()\n",
    "feature_importance = np.abs(first_layer_weights).sum(axis=0)\n",
    "first_layer_weights = model.fc1.weight.data.numpy()\n",
    "if first_layer_weights.size == 0:\n",
    "    print(\"Debug: The first layer weights are empty.\")\n",
    "else:\n",
    "    print(f\"Debug: The first layer weights have a shape of {first_layer_weights.shape}.\")\n",
    "\n",
    "# Debug Statement  Calculate and display feature importance\n",
    "feature_importance = np.abs(first_layer_weights).sum(axis=0)\n",
    "if feature_importance.size == 0:\n",
    "    print(\"Debug: Feature importance calculation resulted in an empty array.\")\n",
    "else:\n",
    "    print(f\"Debug: Feature importance is {feature_importance}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Risk budgeting benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Assuming df_last_10_years contains the daily returns\n",
    "returns = df_last_10_years.drop(columns=df_last_10_years.columns[0])\n",
    "\n",
    "# Initialize list to store optimal weights and returns for each t\n",
    "all_weights = []\n",
    "all_returns = []\n",
    "\n",
    "# Number of assets\n",
    "n_assets = len(returns.columns)\n",
    "\n",
    "# Risk budget for each asset\n",
    "b = np.ones(n_assets) / n_assets  # For example, equal risk budgeting\n",
    "\n",
    "# Arbitrary constant for the constraint\n",
    "c = 1  # Example value, adjust as needed\n",
    "\n",
    "# Start from the 21st observation\n",
    "for t in tqdm(range(54,len(returns) - 1,5)):\n",
    "    # Data up to time t\n",
    "    data_t = returns.iloc[:t+1].astype(\"float16\")\n",
    "    print(data_t.index[-1])\n",
    "    # Covariance matrix of the returns\n",
    "    cov_matrix_values = data_t.cov().values\n",
    "    cov_matrix_values = (cov_matrix_values + cov_matrix_values.T)/2\n",
    "\n",
    "    #cov_matrix_values += np.eye(cov_matrix_values.shape[0],cov_matrix_values.shape[1])\n",
    "\n",
    "    #if min(eigs) <= 0:\n",
    "    #    print(t)\n",
    "    # Portfolio weights as a variable (y)\n",
    "    y = cp.Variable(shape=n_assets)\n",
    "    # Objective function: Minimize the square root of the portfolio variance\n",
    "    objective = cp.Minimize(cp.sqrt(cp.quad_form(y, cp.psd_wrap(cov_matrix_values))))\n",
    "\n",
    "    # Alternative solutions\n",
    "\n",
    "    #objective = cp.Minimize(cp.quad_form(y, cp.psd_wrap(cov_matrix_values)))\n",
    "\n",
    "    #cov_matrix_values = np.array(cov_matrix_values)\n",
    "    #cov_matrix_values += np.eye(cov_matrix_values.shape[0],cov_matrix_values.shape[1])\n",
    "    #L = np.linalg.cholesky(cov_matrix_values)\n",
    "    #objective = cp.Minimize(cp.norm(L@y,2))\n",
    "\n",
    "    # Constraints:\n",
    "    # 1. The weights must sum to 1 (full investment)\n",
    "    # 2. The risk budgeting constraint must be satisfied\n",
    "    # 3. The weights must be non-negative\n",
    "    constraints = [\n",
    "        #cp.sum(y) == 1,        # incompatible with contstraint below. Normalize afterwards\n",
    "        cp.sum(cp.multiply(b, cp.log(y))) >= c,\n",
    "        y >= 1e-5 #strict inequalities are not allowed\n",
    "    ]\n",
    "\n",
    "    # Formulate the optimization problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "\n",
    "    # Solve the problem using a suitable solver\n",
    "    problem.solve(solver=cp.SCS,qcp=True, eps = 1e-5, max_iters  = 100) #kan ange hur mÃ¥nga fÃ¶rsÃ¶ja man vill att solve kan gÃ¶ra. Kolla upp det. \"Maxiteration\". googla pÃ¥ risk budgeting med cvxpy.\n",
    "    # Store the optimal weights for time t\n",
    "    optimal_weights = y.value\n",
    "    #optimal_weights /= np.linalg.norm(optimal_weights,1)\n",
    "    all_weights.append(optimal_weights)\n",
    "\n",
    "    #print(optimal_weights)\n",
    "\n",
    "    #print(type(all_weights))\n",
    "    #print(type(returns.iloc[t+1].values))\n",
    "    \n",
    "    # Calculate and store the portfolio return for time t+1 using weights from time t\n",
    "    next_return = np.dot(returns.iloc[t+1].values, optimal_weights)\n",
    "    all_returns.append(next_return)\n",
    "\n",
    "# Convert lists to arrays for further analysis if needed\n",
    "#all_weights = np.array(all_weights)\n",
    "#all_returns = np.array(all_returns)\n",
    "\n",
    "# Print the first 5 values of all_returns\n",
    "#print(\"First 5 values of all_returns:\")\n",
    "#print(all_returns[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('AP1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41ccadffad69fa272f24ffcf66569b4dc0b054e9043de66efe3af05dc5371c78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
