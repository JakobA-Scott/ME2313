{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate AP1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import pyfolio as pf\n",
    "import empyrical as emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data_nn.xlsx')\n",
    "#df.to_pickle(\"data_nn.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the first column as the date index\n",
    "df.set_index(df.columns[0], inplace=True)\n",
    "\n",
    "# Convert the index to string and then to DatetimeIndex format\n",
    "df.index = pd.to_datetime(df.index.astype(str))\n",
    "\n",
    "# Filter the data for the last ten years\n",
    "df_last_10_years = df.loc[df.index > \"1980-01-02\"]\n",
    "\n",
    "\n",
    "# Apply rolling sum with a window of 252 and require at least 126 non-NaN values\n",
    "df_rolling_sum = df_last_10_years.rolling(window=252, min_periods=int(252//2)).sum()\n",
    "\n",
    "# Forward-fill NaN values, but limit this to a maximum of 5 consecutive fills\n",
    "df_filled = df_last_10_years.ffill(limit=5)\n",
    "\n",
    "# Drop any remaining NaN values that still exist after the forward-fill operation\n",
    "df_cleaned = df_filled.dropna()\n",
    "\n",
    "#return back original name to not interruppt code.\n",
    "df_last_10_years = df_cleaned\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactored_advanced_features(df_returns):\n",
    "    \"\"\"\n",
    "    Refactored computation of advanced financial features to reduce DataFrame fragmentation.\n",
    "    \"\"\"\n",
    "    skew = {}\n",
    "    kurtosis = {}\n",
    "    max_drawdown = {}\n",
    "    volatility = {}\n",
    "    vaR = {}\n",
    "    momentum = {}\n",
    "    avg_return = {}\n",
    "    rsi = {}\n",
    "\n",
    "        \n",
    "        # 1. Skewness\n",
    "    print(\"Skewness\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        skew[window] = df_returns.rolling(window).skew()\n",
    "\n",
    "        # 2. Kurtosis\n",
    "    print(\"Kurtosis\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        kurtosis[window]=df_returns.rolling(window).kurt()\n",
    "    \n",
    "    # 3. Maximum drawdown\n",
    "    print(\"Maximum drawdown\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        max_drawdown[window] = df_returns.rolling(window).apply(emp.max_drawdown, raw=True)\n",
    "    \n",
    "    # 4. Volatility\n",
    "    print(\"Volatility\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        volatility[window] = df_returns.rolling(window).std()*(252**0.5)\n",
    "    \n",
    "    # 5. Value at Risk\n",
    "    print(\"Value at Risk\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        vaR[window] = df_returns.rolling(window).apply(emp.value_at_risk, raw=True)\n",
    "    \n",
    "    # 6. Momentum\n",
    "    print(\"Momentum\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        momentum[window] = df_returns.rolling(window).sum() # ?\n",
    "\n",
    "    print(\"Average Return\")\n",
    "    for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "        avg_return[window] = df_returns.rolling(window).mean()\n",
    "    \n",
    "    return skew, kurtosis, max_drawdown, volatility, vaR, momentum, avg_return\n",
    "\n",
    "# This function reduces DataFrame fragmentation by constructing all columns and concatenating them at once.\n",
    "\n",
    "# LÃ¤s tommys mex hur de gjorde reversal, sen implementera det. Fixa windows size till vad de hade i rapporten.\n",
    "# skew[20].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness\n",
      "Kurtosis\n",
      "Maximum drawdown\n",
      "Volatility\n",
      "Value at Risk\n",
      "Momentum\n",
      "Average Return\n"
     ]
    }
   ],
   "source": [
    "# Call the function and capture the output\n",
    "skew, kurtosis, max_drawdown, volatility, vaR, momentum, avg_return = refactored_advanced_features(df_last_10_years)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">skew_20</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">avg_return_480</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Equities_0</th>\n",
       "      <th>Equities_1</th>\n",
       "      <th>Equities_2</th>\n",
       "      <th>Equities_3</th>\n",
       "      <th>Equities_4</th>\n",
       "      <th>Equities_5</th>\n",
       "      <th>Equities_6</th>\n",
       "      <th>Equities_7</th>\n",
       "      <th>Equities_8</th>\n",
       "      <th>Equities_9</th>\n",
       "      <th>...</th>\n",
       "      <th>Equity_Sector_1</th>\n",
       "      <th>Equity_Sector_2</th>\n",
       "      <th>Equity_Sector_3</th>\n",
       "      <th>Equity_Sector_4</th>\n",
       "      <th>Equity_Sector_5</th>\n",
       "      <th>Equity_Sector_6</th>\n",
       "      <th>Equity_Sector_7</th>\n",
       "      <th>Equity_Sector_8</th>\n",
       "      <th>Equity_Sector_9</th>\n",
       "      <th>Equity_Sector_10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Column1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-02-20</th>\n",
       "      <td>-0.240833</td>\n",
       "      <td>-0.198584</td>\n",
       "      <td>0.748803</td>\n",
       "      <td>-0.034554</td>\n",
       "      <td>0.183471</td>\n",
       "      <td>-0.216372</td>\n",
       "      <td>0.256436</td>\n",
       "      <td>-0.157259</td>\n",
       "      <td>1.524128</td>\n",
       "      <td>0.163826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>-0.000610</td>\n",
       "      <td>0.000152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-21</th>\n",
       "      <td>-0.086253</td>\n",
       "      <td>-0.070400</td>\n",
       "      <td>0.773535</td>\n",
       "      <td>0.011176</td>\n",
       "      <td>0.312154</td>\n",
       "      <td>-0.041571</td>\n",
       "      <td>0.221016</td>\n",
       "      <td>-0.162284</td>\n",
       "      <td>1.521845</td>\n",
       "      <td>-0.296421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-22</th>\n",
       "      <td>-0.128589</td>\n",
       "      <td>-0.062873</td>\n",
       "      <td>0.757504</td>\n",
       "      <td>-0.109515</td>\n",
       "      <td>0.415347</td>\n",
       "      <td>0.052482</td>\n",
       "      <td>0.191846</td>\n",
       "      <td>-0.470390</td>\n",
       "      <td>1.486008</td>\n",
       "      <td>-0.301710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-23</th>\n",
       "      <td>-0.023004</td>\n",
       "      <td>0.106261</td>\n",
       "      <td>0.830820</td>\n",
       "      <td>-0.006954</td>\n",
       "      <td>0.265489</td>\n",
       "      <td>0.092309</td>\n",
       "      <td>0.325509</td>\n",
       "      <td>-0.476984</td>\n",
       "      <td>1.454702</td>\n",
       "      <td>-0.274151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>-0.000627</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-24</th>\n",
       "      <td>0.050596</td>\n",
       "      <td>0.138392</td>\n",
       "      <td>0.864532</td>\n",
       "      <td>0.148482</td>\n",
       "      <td>0.428164</td>\n",
       "      <td>0.081861</td>\n",
       "      <td>0.420396</td>\n",
       "      <td>-0.548086</td>\n",
       "      <td>1.506494</td>\n",
       "      <td>-0.256232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000159</td>\n",
       "      <td>-0.000647</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 3080 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              skew_20                                                         \\\n",
       "           Equities_0 Equities_1 Equities_2 Equities_3 Equities_4 Equities_5   \n",
       "Column1                                                                        \n",
       "2023-02-20  -0.240833  -0.198584   0.748803  -0.034554   0.183471  -0.216372   \n",
       "2023-02-21  -0.086253  -0.070400   0.773535   0.011176   0.312154  -0.041571   \n",
       "2023-02-22  -0.128589  -0.062873   0.757504  -0.109515   0.415347   0.052482   \n",
       "2023-02-23  -0.023004   0.106261   0.830820  -0.006954   0.265489   0.092309   \n",
       "2023-02-24   0.050596   0.138392   0.864532   0.148482   0.428164   0.081861   \n",
       "\n",
       "                                                        ...  avg_return_480  \\\n",
       "           Equities_6 Equities_7 Equities_8 Equities_9  ... Equity_Sector_1   \n",
       "Column1                                                 ...                   \n",
       "2023-02-20   0.256436  -0.157259   1.524128   0.163826  ...        0.000223   \n",
       "2023-02-21   0.221016  -0.162284   1.521845  -0.296421  ...        0.000203   \n",
       "2023-02-22   0.191846  -0.470390   1.486008  -0.301710  ...        0.000189   \n",
       "2023-02-23   0.325509  -0.476984   1.454702  -0.274151  ...        0.000200   \n",
       "2023-02-24   0.420396  -0.548086   1.506494  -0.256232  ...        0.000202   \n",
       "\n",
       "                                                                            \\\n",
       "           Equity_Sector_2 Equity_Sector_3 Equity_Sector_4 Equity_Sector_5   \n",
       "Column1                                                                      \n",
       "2023-02-20        0.001289        0.000093        0.000232        0.000087   \n",
       "2023-02-21        0.001335        0.000089        0.000194        0.000060   \n",
       "2023-02-22        0.001290        0.000056        0.000162        0.000026   \n",
       "2023-02-23        0.001304        0.000074        0.000161        0.000030   \n",
       "2023-02-24        0.001269        0.000031        0.000141        0.000001   \n",
       "\n",
       "                                                                            \\\n",
       "           Equity_Sector_6 Equity_Sector_7 Equity_Sector_8 Equity_Sector_9   \n",
       "Column1                                                                      \n",
       "2023-02-20        0.000005        0.000010       -0.000008       -0.000610   \n",
       "2023-02-21       -0.000026       -0.000004       -0.000074       -0.000646   \n",
       "2023-02-22       -0.000051       -0.000029       -0.000105       -0.000646   \n",
       "2023-02-23       -0.000030        0.000023       -0.000123       -0.000627   \n",
       "2023-02-24       -0.000062        0.000002       -0.000159       -0.000647   \n",
       "\n",
       "                             \n",
       "           Equity_Sector_10  \n",
       "Column1                      \n",
       "2023-02-20         0.000152  \n",
       "2023-02-21         0.000083  \n",
       "2023-02-22         0.000094  \n",
       "2023-02-23         0.000099  \n",
       "2023-02-24         0.000093  \n",
       "\n",
       "[5 rows x 3080 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the feature DataFrames list\n",
    "features_df_list = []\n",
    "\n",
    "# Create individual lists for each feature's DataFrame\n",
    "skew_df_list = [] \n",
    "kurtosis_df_list = []\n",
    "max_drawdown_df_list = []\n",
    "volatility_df_list = []\n",
    "vaR_df_list = []\n",
    "momentum_df_list = []\n",
    "avg_return_df_list = []\n",
    "\n",
    "# Windows configuration\n",
    "windows = [20, 40, 60, 100, 180, 240, 360, 480]\n",
    "\n",
    "# Iterate through each feature dictionary and create a DataFrame\n",
    "for feature_name, feature_dict in [('skew', skew), ('kurtosis', kurtosis), ('max_drawdown', max_drawdown), \n",
    "                                   ('volatility', volatility), ('vaR', vaR), ('momentum', momentum), ('avg_return', avg_return)]:\n",
    "    # Only keep the windows that are present for each feature\n",
    "    relevant_windows = windows if feature_name != 'kurtosis' else windows[:-1]\n",
    "    feature_df = pd.concat({f'{feature_name}_{window}': feature_dict[window] for window in relevant_windows}, axis=1)\n",
    "    \n",
    "    # Append the individual DataFrame to the corresponding feature list\n",
    "    if feature_name == 'skew':\n",
    "        skew_df_list.append(feature_df)\n",
    "    elif feature_name == 'kurtosis':\n",
    "        kurtosis_df_list.append(feature_df)\n",
    "    elif feature_name == 'max_drawdown':\n",
    "        max_drawdown_df_list.append(feature_df)\n",
    "    elif feature_name == 'volatility':\n",
    "        volatility_df_list.append(feature_df)\n",
    "    elif feature_name == 'vaR':\n",
    "        vaR_df_list.append(feature_df)\n",
    "    elif feature_name == 'momentum':\n",
    "        momentum_df_list.append(feature_df)\n",
    "    elif feature_name == 'avg_return':\n",
    "        avg_return_df_list.append(feature_df)\n",
    "    \n",
    "    # Add the DataFrame to the main list\n",
    "    features_df_list.append(feature_df)\n",
    "\n",
    "\n",
    "# Concatenate all feature DataFrames into a single DataFrame\n",
    "features_df = pd.concat(features_df_list, axis=1)\n",
    "\n",
    "# Concatenate all feature DataFrames into a single DataFrame for each feature\n",
    "if len(skew_df_list) > 1:\n",
    "    skew_df = pd.concat(skew_df_list, axis=1)\n",
    "if len(kurtosis_df_list) > 1:\n",
    "    kurtosis_df = pd.concat(kurtosis_df_list, axis=1)\n",
    "if len(max_drawdown_df_list) > 1:\n",
    "    max_drawdown_df = pd.concat(max_drawdown_df_list, axis=1)\n",
    "if len(volatility_df_list) > 1:\n",
    "    volatility_df = pd.concat(volatility_df_list, axis=1)\n",
    "if len(vaR_df_list) > 1:\n",
    "    vaR_df = pd.concat(vaR_df_list, axis=1)\n",
    "if len(momentum_df_list) > 1:\n",
    "    momentum_df = pd.concat(momentum_df_list, axis=1)\n",
    "if len(avg_return_df_list) > 1:\n",
    "    avg_return_df = pd.concat(avg_return_df_list, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# The individual lists for each feature now contain their respective DataFrames\n",
    "# And features_df_list contains all the feature DataFrames\n",
    "# Let's print the first item of each sublist to confirm\n",
    "#print(\"Skew DataFrame:\\n\", skew_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Kurtosis DataFrame:\\n\", kurtosis_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Max Drawdown DataFrame:\\n\", max_drawdown_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Volatility DataFrame:\\n\", volatility_df_list[0].tail(), \"\\n\")\n",
    "#print(\"VaR DataFrame:\\n\", vaR_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Momentum DataFrame:\\n\", momentum_df_list[0].tail(), \"\\n\")\n",
    "#print(\"Average Return DataFrame:\\n\", avg_return_df_list[0].tail(), \"\\n\")\n",
    "\n",
    "# Print the last 5 rows of the combined DataFrame\n",
    "features_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSI(df_returns, window):\n",
    "    \"\"\"\n",
    "    Computes the Relative Strength Index (RSI) for a given window.\n",
    "    \"\"\"\n",
    "    df = df_returns.copy()\n",
    "    df[df >= 0] = 1\n",
    "    df[df < 0] = 0\n",
    "    df = df.rolling(window).mean()*100\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSI skip for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the last RSI value for each window\n",
    "rsi_values = {}\n",
    "\n",
    "# Calculate RSI for each window and store the last value\n",
    "for window in [20, 40, 60, 100, 180, 240, 360, 480]:\n",
    "    rsi_df = RSI(df_last_10_years, window)  # df_returns is your DataFrame with returns data\n",
    "    last_rsi_value = rsi_df.iloc[-1]  # Get the last row of the RSI DataFrame\n",
    "    rsi_values[window] = last_rsi_value  # Store it in the dictionary with the window as the key\n",
    "\n",
    "# Print the last RSI value for a 20-day window\n",
    "print(\"Last RSI value for 20-day window:\")\n",
    "print(rsi_values[20])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forming the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the assets and windows outside of the function for clarity\n",
    "assets = [\n",
    "    'Equities_0', 'Equities_1', 'Equities_2', 'Equities_3', 'Equities_4', 'Equities_5', 'Equities_6', 'Equities_7',\n",
    "    'Equities_8', 'Equities_9', 'Equities_10', 'Equities_11', 'Equities_12', 'Equities_13', 'Equities_14', 'Equities_15',\n",
    "    'Equities_16', 'FX_0', 'FX_1', 'FX_2', 'FX_3', 'FX_4', 'FX_5', 'FX_6', 'FX_7', 'FX_8', 'FX_9', 'FX_10', 'FX_11',\n",
    "    'FX_12', 'FX_13', 'Bonds_0', 'Bonds_1', 'Bonds_2', 'Bonds_3', 'Bonds_4', 'Bonds_5', 'Bonds_6', 'Bonds_7', 'Bonds_8',\n",
    "    'Bonds_9', 'Bonds_10', 'Bonds_11', 'Bonds_12', 'Bonds_13', 'Equity_Sector_0', 'Equity_Sector_1', 'Equity_Sector_2',\n",
    "    'Equity_Sector_3', 'Equity_Sector_4', 'Equity_Sector_5', 'Equity_Sector_6', 'Equity_Sector_7', 'Equity_Sector_8',\n",
    "    'Equity_Sector_9', 'Equity_Sector_10'\n",
    "]\n",
    "windows = [20, 40, 60, 100, 180, 240, 360, 480]\n",
    "\n",
    "# Generate the final DataFrame\n",
    "final_rows = []\n",
    "for date in df_last_10_years.index:\n",
    "    for asset in assets:\n",
    "        row = [date, asset]\n",
    "        for feature_name, feature_dict in [('skew', skew), ('kurtosis', kurtosis), ('max_drawdown', max_drawdown), \n",
    "                                           ('volatility', volatility), ('vaR', vaR), ('momentum', momentum), \n",
    "                                           ('avg_return', avg_return)]:\n",
    "            for window in windows:\n",
    "                # Check if the window exists for this feature, if not, use NaN\n",
    "                value = feature_dict[window].loc[date, asset] if window in feature_dict else float('nan')\n",
    "                row.append(value)\n",
    "        final_rows.append(row)\n",
    "\n",
    "# Define the column names for the final DataFrame\n",
    "column_names = ['Date', 'Asset']\n",
    "for feature_name in ['skew', 'kurtosis', 'max_drawdown', 'volatility', 'vaR', 'momentum', 'avg_return']:\n",
    "    for window in windows:\n",
    "        column_names.extend([f'{feature_name}_{window}'])\n",
    "\n",
    "# Now create the DataFrame\n",
    "final_df = pd.DataFrame(final_rows, columns=column_names)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "print(final_df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN - Model free revised\n",
    "Key Changes:\n",
    "1. Updated Sharpe Ratio Loss: The loss function now accepts both the weights output by the model and the target returns for the next day, using these to calculate the portfolio returns and then the Sharpe Ratio.\n",
    "\n",
    "2. Target Returns Preparation: The target returns for the next day (target_returns_for_next_day) are prepared by shifting the returns data.\n",
    "\n",
    "3. Training Loop Adjustments: In the training loop, the loss is calculated using both the weights and the target returns for the next day.\n",
    "\n",
    "4. Data Splitting and Scaling: The data is split into training and test sets, with the target returns aligned with the feature data.\n",
    "\n",
    "5. Splitting the data over time instead of doing it randomly\n",
    "\n",
    "6. Changed y to y_test in portfolio_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/216 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_TensorBase.item() takes no arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     90\u001b[0m weights \u001b[38;5;241m=\u001b[39m model(X_train_tensor)\n\u001b[0;32m---> 91\u001b[0m a  \u001b[38;5;241m=\u001b[39m y_train_tensor\u001b[38;5;241m.\u001b[39mitem(d)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     93\u001b[0m sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: _TensorBase.item() takes no arguments (1 given)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Define the neural network model\n",
    "class MultivariateNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MultivariateNN, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.input_layer(x))\n",
    "        x = F.softmax(self.hidden_layer(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Custom Sharpe Ratio Loss\n",
    "class SharpeRatioLoss(nn.Module):\n",
    "    def __init__(self, risk_free_rate=0):\n",
    "        super(SharpeRatioLoss, self).__init__()\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "\n",
    "    def forward(self, weights, target_returns):\n",
    "        print(weights.shape)\n",
    "        print(target_returns.shape)\n",
    "        sys.exit(\"f\")\n",
    "        portfolio_returns = (weights * target_returns)\n",
    "        expected_return = portfolio_returns.mean()\n",
    "        std_dev_return = portfolio_returns.std()\n",
    "        sharpe_ratio = (expected_return - self.risk_free_rate) / (std_dev_return + 1e-6)\n",
    "        return -sharpe_ratio\n",
    "\n",
    "# Assuming 'feature_df' is your dataset as a pandas DataFrame\n",
    "calculated_features_df = pd.DataFrame(feature_df)\n",
    "features_df = calculated_features_df.fillna(calculated_features_df.mean())\n",
    "X = features_df\n",
    "y = df_last_10_years.drop(columns=df_last_10_years.columns[0])\n",
    "\n",
    "start_date = '2019-01-01' # Random date\n",
    "look_ahead = 5 # Chosen arbitrarily\n",
    "# The target is also 5 days ahead\n",
    "target_y = y.shift(-look_ahead).dropna(how = 'all')\n",
    "\n",
    "\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=target_y.index[-2], freq=f'{look_ahead}B')\n",
    "for d in tqdm(date_range):\n",
    "    X_train = X.loc[d - pd.offsets.BDay(int(400)):d-pd.offsets.BDay(2)] # Lagging by 2 days\n",
    "    y_train = target_y.loc[d - pd.offsets.BDay(int(400)):d-pd.offsets.BDay(2)]\n",
    "    # X_test should now represent \"holding a portfolio for 5 days\"\n",
    "    X_test = X.loc[d:d+pd.offsets.BDay(look_ahead-1)] # This is 5 days forward\n",
    "    y_test = target_y.loc[d:d+pd.offsets.BDay(look_ahead-1)]\n",
    "    \n",
    "    # Scaling the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Converting to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.values)\n",
    "    y_test_tensor = torch.FloatTensor(y_test.values)\n",
    "    \n",
    "    # Initialize the neural network model\n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "    hidden_dim = 32\n",
    "    output_dim = y_train_tensor.shape[1]\n",
    "    model = MultivariateNN(input_dim, hidden_dim, output_dim)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = SharpeRatioLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001) # Adjusted learning rate based on feedback\n",
    "    \n",
    "\n",
    "    # Training loop\n",
    "    epochs = 1\n",
    "    weights_over_time = []\n",
    "    sharpe_ratios_over_time = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        weights = model(X_train_tensor)\n",
    "        loss = criterion(weights, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(X_test_tensor) #5x55\n",
    "        test_loss = criterion(test_output, y_test_tensor) #5x55\n",
    "        weights_over_time.append(test_output.cpu().numpy())\n",
    "        sharpe_ratios_over_time.append(-test_loss.item())\n",
    "    \n",
    "    # Here you should add code to save the model, track performance, etc.\n",
    "\n",
    "\n",
    "# Plot Sharpe Ratio over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(sharpe_ratios_over_time, label=\"Sharpe Ratio over time\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Sharpe Ratio\")\n",
    "plt.title(\"Sharpe Ratio Over time\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot weights over time for a specific asset\n",
    "asset_index = 0  # Adjust this index to plot weights for different assets\n",
    "weights_of_asset = [weights[epoch][asset_index].detach().numpy() for epoch in range(epochs)]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(weights_of_asset, label=f\"Weights of Asset {asset_index} Over Epocs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.title(f\"Weight of Asset {asset_index} Over Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    neural_network_output = model(X_test_tensor).cpu().numpy()\n",
    "    neural_network_output_reshaped = neural_network_output.reshape(-1, neural_network_output.shape[-1])\n",
    "    normalized_predicted_weights = neural_network_output_reshaped / np.sum(neural_network_output_reshaped, axis=1, keepdims=True)\n",
    "\n",
    "# Convert the normalized predicted weights to DataFrame\n",
    "weights_df = pd.DataFrame(normalized_predicted_weights, columns=y.columns)\n",
    "if len(weights_df) == len(y_test):\n",
    "    weights_df.index = y_test.index\n",
    "else:\n",
    "    sys.exit(\"Lengths of weights and test data do not match\")\n",
    "\"\"\"\n",
    "# Calculate weighted returns\n",
    "portfolio_returns = weights_df.shift(1).mul(y_test).dropna(how='all')\n",
    "cumulative_returns = portfolio_returns.sum(axis=1).cumsum()\n",
    "\n",
    "# Plot cumulative portfolio returns\n",
    "cumulative_returns.plot(title='Cumulative Portfolio Returns - Test Set')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Cumulative Returns\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN - Model Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SharpeRatioLoss.forward() missing 1 required positional argument: 'target_returns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     77\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_train_tensor)\n\u001b[0;32m---> 78\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs)\n\u001b[1;32m     79\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     80\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/AP1/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: SharpeRatioLoss.forward() missing 1 required positional argument: 'target_returns'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "class RiskBudgetingLayer(nn.Module):\n",
    "    def __init__(self, n_assets, risk_budgets):\n",
    "        super(RiskBudgetingLayer, self).__init__()\n",
    "        self.n_assets = n_assets\n",
    "        self.risk_budgets = risk_budgets\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate covariance matrix\n",
    "        covariance_matrix = torch.cov(x.T).detach().cpu().numpy()\n",
    "        assert covariance_matrix.shape[0] == covariance_matrix.shape[1], \"Covariance matrix is not square\"\n",
    "\n",
    "        # Ensure positive semi-definiteness of the covariance matrix\n",
    "        covariance_matrix = (covariance_matrix + covariance_matrix.T) / 2\n",
    "        eigenvalues, _ = np.linalg.eigh(covariance_matrix)\n",
    "        covariance_matrix += np.eye(self.n_assets) * np.maximum(0, -eigenvalues.min() + 1e-6)\n",
    "\n",
    "        # Convex optimization\n",
    "        y = cp.Variable(self.n_assets)\n",
    "        objective = cp.Minimize(cp.sqrt(cp.quad_form(y, covariance_matrix)))\n",
    "        constraints = [cp.sum(cp.multiply(self.risk_budgets, cp.log(y))) >= 1, y >= 1e-5]\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        problem.solve(solver=cp.SCS, qcp=True, eps=1e-5, max_iters=100)\n",
    "        optimized_allocation = y.value\n",
    "\n",
    "        # Check for invalid values and handle them\n",
    "        if optimized_allocation is None or np.any(np.isnan(optimized_allocation)):\n",
    "            optimized_allocation = np.ones(self.n_assets) / self.n_assets  # Fallback to equal allocation\n",
    "\n",
    "        # Convert back to PyTorch tensor with a differentiable operation\n",
    "        optimized_allocation_tensor = torch.tensor(optimized_allocation, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "        # Apply softmax to ensure the output sums to 1\n",
    "        return F.softmax(optimized_allocation_tensor * 1.0, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "class MultivariateNNWithRiskBudgeting(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_assets, risk_budgets):\n",
    "        super(MultivariateNNWithRiskBudgeting, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.risk_budgeting_layer = RiskBudgetingLayer(n_assets, risk_budgets)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.input_layer(x))\n",
    "        x = F.softmax(self.hidden_layer(x), dim=1)\n",
    "        x = self.risk_budgeting_layer(x)\n",
    "        return x\n",
    "\n",
    "# Assuming the data and necessary variables are already defined as per your notebook\n",
    "# Example usage\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 55\n",
    "n_assets = y_train.shape[1]\n",
    "risk_budgets = np.ones(n_assets) / n_assets  # Example risk budgets, equal for each asset\n",
    "\n",
    "model = MultivariateNNWithRiskBudgeting(input_dim, hidden_dim, n_assets, risk_budgets)\n",
    "\n",
    "# Continue with the existing training loop and loss function\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = SharpeRatioLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Optional: Print loss every N epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Extracting only the weights (and biases, if needed) of the output layer\n",
    "output_layer_weights = model.hidden_layer.weight.data.cpu().numpy()\n",
    "output_layer_biases = model.hidden_layer.bias.data.cpu().numpy()\n",
    "\n",
    "# You can now use output_layer_weights and output_layer_biases as needed\n",
    "print(\"Output Layer Weights:\", output_layer_weights)\n",
    "print(\"Output Layer Biases:\", output_layer_biases)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n",
    "    neural_network_output = model(X_test_tensor).cpu().numpy()\n",
    "# Now neural_network_output contains the output of the neural network\n",
    "print(\"Neural Network Output:\", neural_network_output)\n",
    "##print the dimensions of neural network\n",
    "print(\"Neural Network Output shape:\", neural_network_output.shape)\n",
    "#with torch.no_grad():\n",
    "    #train_outputs = model(X_train_tensor)\n",
    "    #test_outputs = model(X_test_tensor)\n",
    "    #train_loss = criterion(train_outputs)\n",
    "    #test_loss = criterion(test_outputs)\n",
    "    #print(f\"Final Training Loss: {train_loss.item()}\")\n",
    "    #print(f\"Final Test Loss: {test_loss.item()}\")\n",
    "\n",
    "print(\"Shape of neural network output:\", neural_network_output.shape)\n",
    "\n",
    "# Normalize the predicted weights\n",
    "normalized_predicted_weights = neural_network_output / np.sum(neural_network_output)\n",
    "\n",
    "# Convert the normalized predicted weights to DataFrame\n",
    "weights_df = pd.DataFrame([normalized_predicted_weights], columns=returns.columns)\n",
    "\n",
    "# Replicate the weights across the dates in the returns DataFrame\n",
    "weights_df = pd.concat([weights_df]*len(returns), ignore_index=True)\n",
    "weights_df.index = returns.index\n",
    "\n",
    "# Calculate weighted returns\n",
    "portfolio_returns = weights_df.shift(1).mul(returns).dropna(how='all')\n",
    "\n",
    "# Calculate cumulative portfolio returns and plot\n",
    "cumulative_returns = portfolio_returns.sum(axis=1).cumsum()\n",
    "cumulative_returns.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN - Model Based Revised, not done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom Sharpe Ratio Loss (same as in the model-free approach)\n",
    "class SharpeRatioLoss(nn.Module):\n",
    "    def __init__(self, risk_free_rate=0):\n",
    "        super(SharpeRatioLoss, self).__init__()\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "\n",
    "    def forward(self, weights, target_returns):\n",
    "        # Calculate portfolio returns\n",
    "        portfolio_returns = (weights * target_returns).sum(dim=1)\n",
    "        # Calculate Sharpe Ratio\n",
    "        expected_return = portfolio_returns.mean()\n",
    "        std_dev_return = portfolio_returns.std()\n",
    "        sharpe_ratio = (expected_return - self.risk_free_rate) / (std_dev_return + 1e-6)\n",
    "        return -sharpe_ratio\n",
    "\n",
    "# Risk Budgeting Layer using cvxpy\n",
    "class RiskBudgetingLayer(nn.Module):\n",
    "    def __init__(self, num_assets, c_value):\n",
    "        super(RiskBudgetingLayer, self).__init__()\n",
    "        self.num_assets = num_assets\n",
    "        self.c_value = c_value\n",
    "\n",
    "    def forward(self, cov_matrix):\n",
    "        # Define the optimization problem for risk budgeting\n",
    "        weights = cp.Variable(self.num_assets)\n",
    "        objective = cp.Minimize(cp.quad_form(weights, cov_matrix))\n",
    "        constraints = [cp.sum(weights) == 1, weights >= 0]  # Example constraints\n",
    "        problem = cp.Problem(objective, constraints)\n",
    "        cvxpy_layer = CvxpyLayer(problem, parameters=[cov_matrix], variables=[weights])\n",
    "        optimized_weights, = cvxpy_layer(cov_matrix)\n",
    "        return optimized_weights\n",
    "\n",
    "# Neural Network with Risk Budgeting Layer\n",
    "class ModelBased(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_assets, c_value):\n",
    "        super(ModelBased, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_assets)\n",
    "        self.risk_budget_layer = RiskBudgetingLayer(num_assets, c_value)\n",
    "        \n",
    "    def forward(self, x, cov_matrix):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        optimized_weights = self.risk_budget_layer(cov_matrix)\n",
    "        return optimized_weights\n",
    "\n",
    "# Assuming 'feature_df' and 'df_last_10_years' are defined as per your data\n",
    "calculated_features_df = pd.DataFrame(feature_df)\n",
    "features_df = calculated_features_df.fillna(calculated_features_df.mean())\n",
    "X = features_df\n",
    "y = df_last_10_years.drop(columns=df_last_10_years.columns[0])  # Assuming this is the returns data\n",
    "\n",
    "# Prepare target returns for the next day (for Sharpe Ratio Loss)\n",
    "target_returns_for_next_day = y.shift(-1).fillna(0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target_returns_for_next_day, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values)\n",
    "\n",
    "# Initialize the model-based neural network\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "hidden_dim = 32  # Example hidden dimension\n",
    "num_assets = y_train_tensor.shape[1]\n",
    "c_value = 1  # Example c_value for risk budgeting\n",
    "\n",
    "model = ModelBased(input_dim, hidden_dim, num_assets, c_value)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = SharpeRatioLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Example covariance matrix calculation (replace with actual data)\n",
    "    cov_matrix = torch.rand(num_assets, num_assets)  # Replace with actual covariance matrix\n",
    "\n",
    "    # Forward pass (passing covariance matrix to the model)\n",
    "    portfolio_weights = model(X_train_tensor, cov_matrix)\n",
    "    loss = criterion(portfolio_weights, y_train_tensor)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Assuming test data is prepared similarly to training data\n",
    "    cov_matrix_test = torch.rand(num_assets, num_assets)  # Replace with actual test covariance matrix\n",
    "    neural_network_output = model(X_test_tensor, cov_matrix_test).cpu().numpy()\n",
    "\n",
    "# Normalize the predicted weights\n",
    "normalized_predicted_weights = neural_network_output / np.sum(neural_network_output, axis=1, keepdims=True)\n",
    "\n",
    "# Convert the normalized predicted weights to DataFrame\n",
    "weights_df = pd.DataFrame(normalized_predicted_weights, columns=y_test.columns)\n",
    "\n",
    "# Align the weights DataFrame with the test returns DataFrame\n",
    "# Check if the lengths match\n",
    "if len(weights_df) == len(y_test):\n",
    "    weights_df.index = y_test.index\n",
    "else:\n",
    "    # Truncate the returns DataFrame to match the length of the weights DataFrame\n",
    "    truncated_y_test = y_test.iloc[:len(weights_df)]\n",
    "    weights_df.index = truncated_y_test.index\n",
    "\n",
    "# Calculate weighted returns for the test set\n",
    "portfolio_returns_test = weights_df.shift(1).mul(truncated_y_test).dropna(how='all')\n",
    "\n",
    "# Calculate cumulative portfolio returns and plot\n",
    "cumulative_returns_test = portfolio_returns_test.sum(axis=1).cumsum()\n",
    "cumulative_returns_test.plot(title='Cumulative Portfolio Returns - Test Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Risk budgeting benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AP1 risk budgetering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 154/154 [07:45<00:00,  3.02s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "# Assuming df_last_10_years contains the daily returns\n",
    "returns = df_last_10_years.drop(columns=df_last_10_years.columns[0])\n",
    "n_assets = len(returns.columns)\n",
    "# Define a function that will be parallelized\n",
    "def optimize_weights(t, returns, n_assets):\n",
    "    b = np.ones(n_assets) / n_assets  # For example, equal risk budgeting\n",
    "    c = 1\n",
    "    # Code that was originally in your for-loop goes here\n",
    "    # For example:\n",
    "    data_t = returns.iloc[:t]\n",
    "    cov_matrix_values = data_t.cov().values\n",
    "    cov_matrix_values = (cov_matrix_values + cov_matrix_values.T)/2\n",
    "    y = cp.Variable(shape=n_assets)\n",
    "    # Objective function: Minimize the square root of the portfolio variance\n",
    "    objective = cp.Minimize(cp.sqrt(cp.quad_form(y, cp.psd_wrap(cov_matrix_values))))\n",
    "    constraints = [\n",
    "        cp.sum(cp.multiply(b, cp.log(y))) >= c,\n",
    "        y >= 1e-5 #strict inequalities are not allowed\n",
    "    ]\n",
    "    # Formulate the optimization problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    # Solve the problem using a suitable solver\n",
    "    problem.solve(solver=cp.SCS,qcp=True, eps = 1e-5, max_iters  = 100) \n",
    "\n",
    "    # Extract the results\n",
    "    optimal_weights = y.value\n",
    "    date = data_t.index[-1]\n",
    "    # Return the results for this iteration\n",
    "    return (date, optimal_weights)\n",
    "\n",
    "# Precompute any variables that don't change inside the loop\n",
    "# ...\n",
    "\n",
    "# Set up the joblib parallelization\n",
    "# Here, 'range(len(returns))' is the range over which you want to parallelize\n",
    "# results = Parallel(n_jobs=-1)(delayed(optimize_weights)(t, returns, n_assets) for t in tqdm(range(54, len(returns))))\n",
    "# Set up the joblib parallelization with tqdm\n",
    "results = Parallel(n_jobs=-1)(delayed(optimize_weights)(t, returns, n_assets) for t in tqdm(range(54, len(returns), 5))\n",
    ")\n",
    "# After parallelization, recombine the results as necessary\n",
    "# For example:\n",
    "# Create a dictionary with dates as keys and optimal weights as values\n",
    "optimal_weights_dict = {date: weights for date, weights in results}\n",
    "\n",
    "w = pd.DataFrame.from_dict(optimal_weights_dict, orient='index', columns=returns.columns)\n",
    "# Normalize the weights\n",
    "w = w.div(w.sum(axis=1), axis=0)\n",
    "# Calculate the portfolio returns\n",
    "portfolio_returns = w.shift(1).mul(returns).dropna(how='all')\n",
    "portfolio_returns.sum(axis=1).cumsum().plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('AP1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41ccadffad69fa272f24ffcf66569b4dc0b054e9043de66efe3af05dc5371c78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
